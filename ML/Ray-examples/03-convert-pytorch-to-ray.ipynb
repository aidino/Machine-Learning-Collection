{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert existing PyTorch code to Ray AIR\n",
    "\n",
    "Official pytorch tutorial: https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unmodified PyTorch code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(\"Using {} device\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader: DataLoader, model: nn.Module, loss_fn: nn.Module, optimizer: torch.optim.Optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch*len(X)\n",
    "            print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_epoch(dataloader: DataLoader, model: nn.Module, loss_fn: nn.Module):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \n",
      " -----------------------------------------\n",
      "loss: 2.320704 [    0/60000]\n",
      "loss: 2.300536 [ 6400/60000]\n",
      "loss: 2.280033 [12800/60000]\n",
      "loss: 2.263206 [19200/60000]\n",
      "loss: 2.254644 [25600/60000]\n",
      "loss: 2.229671 [32000/60000]\n",
      "loss: 2.230517 [38400/60000]\n",
      "loss: 2.202650 [44800/60000]\n",
      "loss: 2.197895 [51200/60000]\n",
      "loss: 2.160965 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 2.155929 \n",
      "\n",
      "Epoch 2 \n",
      " -----------------------------------------\n",
      "loss: 2.171209 [    0/60000]\n",
      "loss: 2.158050 [ 6400/60000]\n",
      "loss: 2.098881 [12800/60000]\n",
      "loss: 2.110374 [19200/60000]\n",
      "loss: 2.064276 [25600/60000]\n",
      "loss: 2.007083 [32000/60000]\n",
      "loss: 2.031127 [38400/60000]\n",
      "loss: 1.954005 [44800/60000]\n",
      "loss: 1.958551 [51200/60000]\n",
      "loss: 1.881614 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 59.7%, Avg loss: 1.878456 \n",
      "\n",
      "Epoch 3 \n",
      " -----------------------------------------\n",
      "loss: 1.914828 [    0/60000]\n",
      "loss: 1.881270 [ 6400/60000]\n",
      "loss: 1.761325 [12800/60000]\n",
      "loss: 1.799968 [19200/60000]\n",
      "loss: 1.698061 [25600/60000]\n",
      "loss: 1.643423 [32000/60000]\n",
      "loss: 1.664817 [38400/60000]\n",
      "loss: 1.562739 [44800/60000]\n",
      "loss: 1.584172 [51200/60000]\n",
      "loss: 1.476103 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.3%, Avg loss: 1.495922 \n",
      "\n",
      "Epoch 4 \n",
      " -----------------------------------------\n",
      "loss: 1.566025 [    0/60000]\n",
      "loss: 1.529664 [ 6400/60000]\n",
      "loss: 1.377127 [12800/60000]\n",
      "loss: 1.450011 [19200/60000]\n",
      "loss: 1.333411 [25600/60000]\n",
      "loss: 1.324325 [32000/60000]\n",
      "loss: 1.345264 [38400/60000]\n",
      "loss: 1.263893 [44800/60000]\n",
      "loss: 1.296012 [51200/60000]\n",
      "loss: 1.201066 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.2%, Avg loss: 1.226453 \n",
      "\n",
      "Epoch 5 \n",
      " -----------------------------------------\n",
      "loss: 1.307963 [    0/60000]\n",
      "loss: 1.291042 [ 6400/60000]\n",
      "loss: 1.121078 [12800/60000]\n",
      "loss: 1.229174 [19200/60000]\n",
      "loss: 1.103881 [25600/60000]\n",
      "loss: 1.125383 [32000/60000]\n",
      "loss: 1.159945 [38400/60000]\n",
      "loss: 1.087069 [44800/60000]\n",
      "loss: 1.123265 [51200/60000]\n",
      "loss: 1.050850 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 1.067354 \n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1} \\n -----------------------------------------\")\n",
    "    train_epoch(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_epoch(test_dataloader, model, loss_fn)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"models/01_pytorch_MNIST.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper function for production code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn():\n",
    "    batch_size = 64\n",
    "    lr = 1e-3\n",
    "    epochs = 5\n",
    "\n",
    "    # Create data loaders.\n",
    "    train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "    test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "    # get cpu or gpu device for training\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(\"Using {} device\".format(device))\n",
    "\n",
    "    model = NeuralNetwork().to(device)\n",
    "    print(model)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1} \\n -----------------------------------------\")\n",
    "        train_epoch(train_dataloader, model, loss_fn, optimizer)\n",
    "        test_epoch(test_dataloader, model, loss_fn)\n",
    "    \n",
    "    print(\"Done\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 1 \n",
      " -----------------------------------------\n",
      "loss: 2.308989 [    0/60000]\n",
      "loss: 2.294538 [ 6400/60000]\n",
      "loss: 2.274389 [12800/60000]\n",
      "loss: 2.263218 [19200/60000]\n",
      "loss: 2.257612 [25600/60000]\n",
      "loss: 2.225626 [32000/60000]\n",
      "loss: 2.233253 [38400/60000]\n",
      "loss: 2.201899 [44800/60000]\n",
      "loss: 2.196631 [51200/60000]\n",
      "loss: 2.165219 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 40.3%, Avg loss: 2.161686 \n",
      "\n",
      "Epoch 2 \n",
      " -----------------------------------------\n",
      "loss: 2.169200 [    0/60000]\n",
      "loss: 2.161680 [ 6400/60000]\n",
      "loss: 2.104584 [12800/60000]\n",
      "loss: 2.122696 [19200/60000]\n",
      "loss: 2.079923 [25600/60000]\n",
      "loss: 2.013402 [32000/60000]\n",
      "loss: 2.047680 [38400/60000]\n",
      "loss: 1.963887 [44800/60000]\n",
      "loss: 1.967264 [51200/60000]\n",
      "loss: 1.903233 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 56.9%, Avg loss: 1.898693 \n",
      "\n",
      "Epoch 3 \n",
      " -----------------------------------------\n",
      "loss: 1.921272 [    0/60000]\n",
      "loss: 1.898150 [ 6400/60000]\n",
      "loss: 1.777277 [12800/60000]\n",
      "loss: 1.829894 [19200/60000]\n",
      "loss: 1.721745 [25600/60000]\n",
      "loss: 1.665770 [32000/60000]\n",
      "loss: 1.701103 [38400/60000]\n",
      "loss: 1.590200 [44800/60000]\n",
      "loss: 1.614336 [51200/60000]\n",
      "loss: 1.515571 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Avg loss: 1.529917 \n",
      "\n",
      "Epoch 4 \n",
      " -----------------------------------------\n",
      "loss: 1.586119 [    0/60000]\n",
      "loss: 1.559827 [ 6400/60000]\n",
      "loss: 1.403234 [12800/60000]\n",
      "loss: 1.483986 [19200/60000]\n",
      "loss: 1.366207 [25600/60000]\n",
      "loss: 1.360142 [32000/60000]\n",
      "loss: 1.385897 [38400/60000]\n",
      "loss: 1.301049 [44800/60000]\n",
      "loss: 1.336513 [51200/60000]\n",
      "loss: 1.237655 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.9%, Avg loss: 1.262944 \n",
      "\n",
      "Epoch 5 \n",
      " -----------------------------------------\n",
      "loss: 1.331406 [    0/60000]\n",
      "loss: 1.322640 [ 6400/60000]\n",
      "loss: 1.151797 [12800/60000]\n",
      "loss: 1.260175 [19200/60000]\n",
      "loss: 1.137674 [25600/60000]\n",
      "loss: 1.165605 [32000/60000]\n",
      "loss: 1.193481 [38400/60000]\n",
      "loss: 1.124449 [44800/60000]\n",
      "loss: 1.165183 [51200/60000]\n",
      "loss: 1.075246 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 1.098383 \n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "train_fn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting with Ray: Distribute the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mpp/.conda/envs/ray/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-09-05 16:15:38,160\tINFO util.py:159 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2023-09-05 16:15:38,482\tINFO util.py:159 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import ray.train as train\n",
    "from ray.air import session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader: DataLoader, model: nn.Module, loss_fn: nn.Module, optimizer: torch.optim.Optimizer):\n",
    "    size = len(dataloader.dataset) // session.get_world_size()\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Forward pass\n",
    "        pred = model(X) \n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch*len(X)\n",
    "\n",
    "\n",
    "def test_epoch(dataloader: DataLoader, model: nn.Module, loss_fn: nn.Module):\n",
    "    size = len(dataloader.dataset) // session.get_world_size()\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    return test_loss\n",
    "\n",
    "\n",
    "# Use config dict to configure some hyperparameters\n",
    "def train_func(config: dict):\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    lr = config[\"lr\"]\n",
    "    epochs = config[\"epochs\"]\n",
    "\n",
    "    # Dynamically adjust the worker batch size according to the number of workers: \n",
    "    batch_size_per_worker = batch_size // session.get_world_size()\n",
    "\n",
    "    # Create data loaders.\n",
    "    train_dataloader = DataLoader(training_data, batch_size=batch_size_per_worker)\n",
    "    test_dataloader = DataLoader(test_data, batch_size=batch_size_per_worker)\n",
    "\n",
    "    # Prepare the data loader for distributed data sharding:\n",
    "    train_dataloader = train.torch.prepare_data_loader(train_dataloader)\n",
    "    test_dataloader = train.torch.prepare_data_loader(test_dataloader)\n",
    "\n",
    "    # Prepare the model for distributed gradient updates:\n",
    "    model = NeuralNetwork()\n",
    "    model = train.torch.prepare_model(model)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    for t in range(epochs):\n",
    "        train_epoch(train_dataloader, model, loss_fn, optimizer)\n",
    "        test_loss = test_epoch(test_dataloader, model, loss_fn)\n",
    "        session.report(dict(loss=test_loss))\n",
    "\n",
    "    \n",
    "    print(\"Done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we’ll use Ray Train’s TorchTrainer to kick off the training. Note that we can set the hyperparameters here! In the scaling_config we can also configure how many parallel workers to use and if we want to enable GPU training or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-09-05 16:16:14</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:25.91        </td></tr>\n",
       "<tr><td>Memory:      </td><td>20.6/30.9 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 3.0/20 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status    </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TorchTrainer_cd422_00000</td><td>TERMINATED</td><td>192.168.33.188:77763</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">          22.211</td><td style=\"text-align: right;\">1.05797</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-05 16:15:48,426\tINFO data_parallel_trainer.py:404 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "\u001b[2m\u001b[36m(TrainTrainable pid=77763)\u001b[0m GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "\u001b[2m\u001b[36m(TorchTrainer pid=77763)\u001b[0m GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "\u001b[2m\u001b[36m(TorchTrainer pid=77763)\u001b[0m Starting distributed worker processes: ['77818 (192.168.33.188)', '77819 (192.168.33.188)']\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=77818)\u001b[0m Setting up process group for: env:// [rank=0, world_size=2]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=77818)\u001b[0m Moving model to device: cpu\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=77818)\u001b[0m Wrapping provided model in DistributedDataParallel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=77819)\u001b[0m Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-05 16:16:14,337\tINFO tune.py:1148 -- Total run time: 25.93 seconds (25.91 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last result:  {'loss': 1.0579689832250023, 'timestamp': 1693905372, 'time_this_iter_s': 4.0605480670928955, 'done': True, 'training_iteration': 5, 'trial_id': 'cd422_00000', 'date': '2023-09-05_16-16-12', 'time_total_s': 22.210975885391235, 'pid': 77763, 'hostname': 'fedora', 'node_ip': '192.168.33.188', 'config': {'train_loop_config': {'batch_size': 64, 'lr': 0.001, 'epochs': 5}}, 'time_since_restore': 22.210975885391235, 'iterations_since_restore': 5, 'experiment_tag': '0'}\n"
     ]
    }
   ],
   "source": [
    "from ray.train.torch import TorchTrainer\n",
    "from ray.air.config import ScalingConfig\n",
    "\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=train_func,\n",
    "    train_loop_config={\n",
    "        \"batch_size\": 64,\n",
    "        \"lr\": 1e-3,\n",
    "        \"epochs\": 5\n",
    "    },\n",
    "    scaling_config=ScalingConfig(num_workers=2, use_gpu=False)\n",
    ")\n",
    "\n",
    "result = trainer.fit()\n",
    "print('Last result: ', result.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable checkpointing to retrieve the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.air import Checkpoint\n",
    "\n",
    "def load_data():\n",
    "    # Download training data from open datasets.\n",
    "    training_data = datasets.FashionMNIST(\n",
    "        root=\"data\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=ToTensor(),\n",
    "    )\n",
    "\n",
    "    # Download test data from open datasets.\n",
    "    test_data = datasets.FashionMNIST(\n",
    "        root=\"data\",\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=ToTensor(),\n",
    "    )\n",
    "    return training_data, test_data\n",
    "\n",
    "\n",
    "def train_func(config: dict):\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    lr = config[\"lr\"]\n",
    "    epochs = config[\"epochs\"]\n",
    "    \n",
    "    batch_size_per_worker = batch_size // session.get_world_size()\n",
    "    \n",
    "    training_data, test_data = load_data()  # <- this is new!\n",
    "    \n",
    "    # Create data loaders.\n",
    "    train_dataloader = DataLoader(training_data, batch_size=batch_size_per_worker)\n",
    "    test_dataloader = DataLoader(test_data, batch_size=batch_size_per_worker)\n",
    "    \n",
    "    train_dataloader = train.torch.prepare_data_loader(train_dataloader)\n",
    "    test_dataloader = train.torch.prepare_data_loader(test_dataloader)\n",
    "    \n",
    "    model = NeuralNetwork()\n",
    "    model = train.torch.prepare_model(model)\n",
    "    \n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    \n",
    "    for t in range(epochs):\n",
    "        train_epoch(train_dataloader, model, loss_fn, optimizer)\n",
    "        test_loss = test_epoch(test_dataloader, model, loss_fn)\n",
    "        checkpoint = Checkpoint.from_dict(\n",
    "            dict(epoch=t, model=model.state_dict())\n",
    "        )\n",
    "        session.report(dict(loss=test_loss), checkpoint=checkpoint)\n",
    "\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-09-05 16:28:57</td></tr>\n",
       "<tr><td>Running for: </td><td>00:09:55.16        </td></tr>\n",
       "<tr><td>Memory:      </td><td>18.7/30.9 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 0/20 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TorchTrainer_4091a_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-05 16:20:02,159\tWARNING insufficient_resources_manager.py:163 -- Ignore this message if the cluster is autoscaling. Training has not started in the last 60 seconds. This could be due to the cluster not having enough resources available. You asked for 1.0 CPUs and 20.0 GPUs, but the cluster only has 20.0 CPUs and 1.0 GPUs available. Stop the training and adjust the required resources (e.g. via the `ScalingConfig` or `resources_per_trial`, or `num_workers` for rllib), or add more resources to your cluster.\n",
      "2023-09-05 16:21:02,229\tWARNING insufficient_resources_manager.py:163 -- Ignore this message if the cluster is autoscaling. Training has not started in the last 60 seconds. This could be due to the cluster not having enough resources available. You asked for 1.0 CPUs and 20.0 GPUs, but the cluster only has 20.0 CPUs and 1.0 GPUs available. Stop the training and adjust the required resources (e.g. via the `ScalingConfig` or `resources_per_trial`, or `num_workers` for rllib), or add more resources to your cluster.\n",
      "2023-09-05 16:22:02,303\tWARNING insufficient_resources_manager.py:163 -- Ignore this message if the cluster is autoscaling. Training has not started in the last 60 seconds. This could be due to the cluster not having enough resources available. You asked for 1.0 CPUs and 20.0 GPUs, but the cluster only has 20.0 CPUs and 1.0 GPUs available. Stop the training and adjust the required resources (e.g. via the `ScalingConfig` or `resources_per_trial`, or `num_workers` for rllib), or add more resources to your cluster.\n",
      "2023-09-05 16:23:02,380\tWARNING insufficient_resources_manager.py:163 -- Ignore this message if the cluster is autoscaling. Training has not started in the last 60 seconds. This could be due to the cluster not having enough resources available. You asked for 1.0 CPUs and 20.0 GPUs, but the cluster only has 20.0 CPUs and 1.0 GPUs available. Stop the training and adjust the required resources (e.g. via the `ScalingConfig` or `resources_per_trial`, or `num_workers` for rllib), or add more resources to your cluster.\n",
      "2023-09-05 16:24:02,461\tWARNING insufficient_resources_manager.py:163 -- Ignore this message if the cluster is autoscaling. Training has not started in the last 60 seconds. This could be due to the cluster not having enough resources available. You asked for 1.0 CPUs and 20.0 GPUs, but the cluster only has 20.0 CPUs and 1.0 GPUs available. Stop the training and adjust the required resources (e.g. via the `ScalingConfig` or `resources_per_trial`, or `num_workers` for rllib), or add more resources to your cluster.\n",
      "2023-09-05 16:25:02,539\tWARNING insufficient_resources_manager.py:163 -- Ignore this message if the cluster is autoscaling. Training has not started in the last 60 seconds. This could be due to the cluster not having enough resources available. You asked for 1.0 CPUs and 20.0 GPUs, but the cluster only has 20.0 CPUs and 1.0 GPUs available. Stop the training and adjust the required resources (e.g. via the `ScalingConfig` or `resources_per_trial`, or `num_workers` for rllib), or add more resources to your cluster.\n",
      "2023-09-05 16:26:02,632\tWARNING insufficient_resources_manager.py:163 -- Ignore this message if the cluster is autoscaling. Training has not started in the last 60 seconds. This could be due to the cluster not having enough resources available. You asked for 1.0 CPUs and 20.0 GPUs, but the cluster only has 20.0 CPUs and 1.0 GPUs available. Stop the training and adjust the required resources (e.g. via the `ScalingConfig` or `resources_per_trial`, or `num_workers` for rllib), or add more resources to your cluster.\n",
      "2023-09-05 16:27:02,714\tWARNING insufficient_resources_manager.py:163 -- Ignore this message if the cluster is autoscaling. Training has not started in the last 60 seconds. This could be due to the cluster not having enough resources available. You asked for 1.0 CPUs and 20.0 GPUs, but the cluster only has 20.0 CPUs and 1.0 GPUs available. Stop the training and adjust the required resources (e.g. via the `ScalingConfig` or `resources_per_trial`, or `num_workers` for rllib), or add more resources to your cluster.\n",
      "2023-09-05 16:28:02,792\tWARNING insufficient_resources_manager.py:163 -- Ignore this message if the cluster is autoscaling. Training has not started in the last 60 seconds. This could be due to the cluster not having enough resources available. You asked for 1.0 CPUs and 20.0 GPUs, but the cluster only has 20.0 CPUs and 1.0 GPUs available. Stop the training and adjust the required resources (e.g. via the `ScalingConfig` or `resources_per_trial`, or `num_workers` for rllib), or add more resources to your cluster.\n",
      "2023-09-05 16:28:57,040\tWARNING tune.py:192 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "2023-09-05 16:28:57,044\tINFO tune.py:1148 -- Total run time: 595.17 seconds (595.16 seconds for the tuning loop).\n",
      "2023-09-05 16:28:57,044\tWARNING tune.py:1158 -- Training has been interrupted, but the most recent state was saved.\n",
      "Resume training with: Trainer.restore(path=\"/home/mpp/ray_results/TorchTrainer_2023-09-05_16-19-01\", ...)\n",
      "2023-09-05 16:28:57,045\tWARNING experiment_analysis.py:916 -- Failed to read the results for 1 trials:\n",
      "- /home/mpp/ray_results/TorchTrainer_2023-09-05_16-19-01/TorchTrainer_4091a_00000_0_2023-09-05_16-19-01\n"
     ]
    }
   ],
   "source": [
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=train_func,\n",
    "    train_loop_config={\"lr\": 1e-3, \"batch_size\": 64, \"epochs\": 4},\n",
    "    scaling_config=ScalingConfig(num_workers=20, use_gpu=True),\n",
    ")\n",
    "result = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ray-lightning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
