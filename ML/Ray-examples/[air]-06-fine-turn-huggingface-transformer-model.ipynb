{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune a ðŸ¤— Transformers model\n",
    "\n",
    "Source: https://docs.ray.io/en/latest/ray-air/examples/huggingface_text_classification.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-06 10:26:11,310\tINFO worker.py:1612 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8266 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CPU': 20.0,\n",
      " 'GPU': 1.0,\n",
      " 'accelerator_type:G': 1.0,\n",
      " 'memory': 5915251508.0,\n",
      " 'node:192.168.33.188': 1.0,\n",
      " 'node:__internal_head__': 1.0,\n",
      " 'object_store_memory': 2957625753.0}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import ray\n",
    "\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "\n",
    "ray.init()\n",
    "\n",
    "pprint(ray.available_resources())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, we will run the training with one GPU worker.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = True  # set this to False to run on CPUs\n",
    "num_workers = 1  # set this to number of GPUs/CPUs you want to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning a model on a text classification task\n",
    "\n",
    "[Original source](https://github.com/huggingface/notebooks/blob/6ca682955173cc9d36ffa431ddda505a048cbe80/examples/text_classification.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLUE_TASKS = [\"cola\", \"mnli\", \"mnli-mm\", \"mrpc\", \"qnli\", \"qqp\", \"rte\", \"sst2\", \"stsb\", \"wnli\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"cola\"\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "actual_task = \"mnli\" if task == \"mnli-mm\" else task\n",
    "datasets = load_dataset(\"glue\", actual_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "def load_metric_fn():\n",
    "    return load_metric('glue', actual_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': (8551, 3), 'validation': (1043, 3), 'test': (1063, 3)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': \"Our friends won't buy this analysis, let alone the next one we propose.\",\n",
       " 'label': 1,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get sample data of datasets\n",
    "datasets['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the data with Ray AIR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_to_keys = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mnli-mm\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"qqp\": (\"question1\", \"question2\"),\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-06 10:26:16,706\tWARNING read_api.py:1794 -- You provided a Huggingface DatasetDict which contains multiple datasets. The output of `from_huggingface` is a dictionary of Ray Datasets. To convert just a single Huggingface Dataset to a Ray Dataset, specify a split. For example, `ray.data.from_huggingface(my_dataset_dictionary['train'])`. Available splits are ['train', 'validation', 'test'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': MaterializedDataset(\n",
       "    num_blocks=1,\n",
       "    num_rows=8551,\n",
       "    schema={sentence: string, label: int64, idx: int32}\n",
       " ),\n",
       " 'validation': MaterializedDataset(\n",
       "    num_blocks=1,\n",
       "    num_rows=1043,\n",
       "    schema={sentence: string, label: int64, idx: int32}\n",
       " ),\n",
       " 'test': MaterializedDataset(\n",
       "    num_blocks=1,\n",
       "    num_rows=1063,\n",
       "    schema={sentence: string, label: int64, idx: int32}\n",
       " )}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray.data \n",
    "\n",
    "ray_datasets = ray.data.from_huggingface(datasets)\n",
    "ray_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ray.data.preprocessors import BatchMapper\n",
    "\n",
    "def preprocess_function(examples: pd.DataFrame):\n",
    "    # if we only have one column, we are inferring.\n",
    "    # no need to tokenize in that case.\n",
    "    if len(examples.columns) == 1:\n",
    "        return examples\n",
    "    \n",
    "    examples = examples.to_dict(\"list\")\n",
    "    sentence1_key, sentence2_key = task_to_keys[task]\n",
    "    if sentence2_key is None:\n",
    "        ret = tokenizer(examples[sentence1_key], truncation=True)\n",
    "    else:\n",
    "        ret = tokenizer(examples[sentence1_key], examples[sentence2_key], truncation=True)\n",
    "    # Add back the original columns\n",
    "    ret = {**examples, **ret}\n",
    "    return pd.DataFrame.from_dict(ret)\n",
    "\n",
    "batch_encoder = BatchMapper(preprocess_function, batch_format=\"pandas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning the model with Ray AIR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-06 10:26:17.754960: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-06 10:26:17.781166: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-06 10:26:18.154673: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "num_labels = 3 if task.startswith(\"mnli\") else 1 if task==\"stsb\" else 2\n",
    "metric_name = \"pearson\" if task == \"stsb\" else \"matthews_correlation\" if task == \"cola\" else \"accuracy\"\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "validation_key = \"validation_mismatched\" if task == \"mnli-mm\" else \"validation_matched\" if task == \"mnli\" else \"validation\"\n",
    "name = f\"{model_name}-finetuned-{task}\"\n",
    "\n",
    "def trainer_init_per_worker(train_dataset, eval_dataset = None, **config):\n",
    "    print(f\"Is CUDA available: {torch.cuda.is_available()}\")\n",
    "    metric = load_metric_fn()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)\n",
    "    args = TrainingArguments(\n",
    "        name,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        learning_rate=config.get(\"learning_rate\", 2e-5),\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=config.get(\"epochs\", 2),\n",
    "        weight_decay=config.get(\"weight_decay\", 0.01),\n",
    "        push_to_hub=False,\n",
    "        disable_tqdm=True,  # declutter the output a little\n",
    "        no_cuda=not use_gpu,  # you need to explicitly set no_cuda if you want CPUs\n",
    "    )\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        if task != \"stsb\":\n",
    "            predictions = np.argmax(predictions, axis=1)\n",
    "        else:\n",
    "            predictions = predictions[:, 0]\n",
    "        return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model,\n",
    "        args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    print(\"Starting training\")\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-06 10:26:18,862\tWARNING base_trainer.py:205 -- The `preprocessor` arg to Trainer is deprecated. Apply preprocessor transformations ahead of time by calling `preprocessor.transform(ds)`. Support for the preprocessor arg will be dropped in a future release.\n"
     ]
    }
   ],
   "source": [
    "from ray.train.huggingface import TransformersTrainer\n",
    "from ray.air.config import RunConfig, ScalingConfig, CheckpointConfig\n",
    "from ray.air.integrations.mlflow import MLflowLoggerCallback\n",
    "\n",
    "trainer = TransformersTrainer(\n",
    "    trainer_init_per_worker=trainer_init_per_worker,\n",
    "    scaling_config=ScalingConfig(num_workers=num_workers, use_gpu=use_gpu),\n",
    "    datasets={\n",
    "        \"train\": ray_datasets[\"train\"],\n",
    "        \"evaluation\": ray_datasets[validation_key],\n",
    "    },\n",
    "    run_config=RunConfig(\n",
    "        callbacks=[MLflowLoggerCallback(experiment_name=name)],\n",
    "        checkpoint_config=CheckpointConfig(\n",
    "            num_to_keep=1,\n",
    "            checkpoint_score_attribute=\"eval_loss\",\n",
    "            checkpoint_score_order=\"min\",\n",
    "        ),\n",
    "    ),\n",
    "    preprocessor=batch_encoder,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-09-06 10:28:01</td></tr>\n",
       "<tr><td>Running for: </td><td>00:01:42.64        </td></tr>\n",
       "<tr><td>Memory:      </td><td>23.6/30.9 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 1.0/20 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  loss</th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">  epoch</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TransformersTrainer_24dc3_00000</td><td>TERMINATED</td><td>192.168.33.188:39990</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         97.6608</td><td style=\"text-align: right;\">0.3884</td><td style=\"text-align: right;\">              0</td><td style=\"text-align: right;\">      2</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Malformed experiment 'mlruns'. Detailed error Yaml file '/home/mpp/Documents/Machine-Learning-Collection/ML/Ray-examples/mlruns/mlruns/meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mpp/.conda/envs/ray/lib/python3.9/site-packages/mlflow/store/tracking/file_store.py\", line 304, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"/home/mpp/.conda/envs/ray/lib/python3.9/site-packages/mlflow/store/tracking/file_store.py\", line 397, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/mpp/.conda/envs/ray/lib/python3.9/site-packages/mlflow/store/tracking/file_store.py\", line 1306, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/mpp/.conda/envs/ray/lib/python3.9/site-packages/mlflow/store/tracking/file_store.py\", line 1299, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/mpp/.conda/envs/ray/lib/python3.9/site-packages/mlflow/utils/file_utils.py\", line 282, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/mpp/Documents/Machine-Learning-Collection/ML/Ray-examples/mlruns/mlruns/meta.yaml' does not exist.\n",
      "WARNING:root:Malformed experiment 'mlruns'. Detailed error Yaml file '/home/mpp/Documents/Machine-Learning-Collection/ML/Ray-examples/mlruns/mlruns/meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mpp/.conda/envs/ray/lib/python3.9/site-packages/mlflow/store/tracking/file_store.py\", line 304, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"/home/mpp/.conda/envs/ray/lib/python3.9/site-packages/mlflow/store/tracking/file_store.py\", line 397, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/mpp/.conda/envs/ray/lib/python3.9/site-packages/mlflow/store/tracking/file_store.py\", line 1306, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/mpp/.conda/envs/ray/lib/python3.9/site-packages/mlflow/store/tracking/file_store.py\", line 1299, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/mpp/.conda/envs/ray/lib/python3.9/site-packages/mlflow/utils/file_utils.py\", line 282, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/mpp/Documents/Machine-Learning-Collection/ML/Ray-examples/mlruns/mlruns/meta.yaml' does not exist.\n",
      "\u001b[2m\u001b[36m(pid=39990)\u001b[0m 2023-09-06 10:26:20.601717: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[2m\u001b[36m(pid=39990)\u001b[0m 2023-09-06 10:26:20.627007: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[2m\u001b[36m(pid=39990)\u001b[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(pid=39990)\u001b[0m 2023-09-06 10:26:21.057493: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[2m\u001b[36m(TransformersTrainer pid=39990)\u001b[0m The `preprocessor` arg to Trainer is deprecated. Apply preprocessor transformations ahead of time by calling `preprocessor.transform(ds)`. Support for the preprocessor arg will be dropped in a future release.\n",
      "\u001b[2m\u001b[36m(TransformersTrainer pid=39990)\u001b[0m Starting distributed worker processes: ['40033 (192.168.33.188)']\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=40033)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=40033)\u001b[0m /home/mpp/.conda/envs/ray/lib/python3.9/site-packages/ray/train/huggingface/transformers/_transformers_utils.py:94: FutureWarning: 'format_type' is deprecated and will be removed in the next major version of datasets. Please use 'formatting=FormattingConfig(format_type=format_type)' instead.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=40033)\u001b[0m   iterable_dataset = datasets.iterable_dataset.IterableDataset(\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=40033)\u001b[0m Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[MapBatches(BatchMapper._transform_pandas)] -> AllToAllOperator[RandomizeBlockOrder]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=40033)\u001b[0m Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=40033)\u001b[0m Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a25fac09ce664f018460a6c8b5e32f4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=40033) - RandomizeBlockOrder 1:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65dafaf0c013433499eee1bd5e0cbe8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=40033) Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=40033)\u001b[0m Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[MapBatches(BatchMapper._transform_pandas)] -> AllToAllOperator[RandomizeBlockOrder]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=40033)\u001b[0m Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=40033)\u001b[0m Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=40033)\u001b[0m /tmp/ipykernel_39094/3612397856.py:4: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=40033)\u001b[0m 2023-09-06 10:26:23.536698: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=40033)\u001b[0m 2023-09-06 10:26:23.561834: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=40033)\u001b[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e5fd11ac24f4188a5a991474947d2a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=40033) - RandomizeBlockOrder 1:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cdc661113874da89b23576ea6285cdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=40033) Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=40033)\u001b[0m Is CUDA available: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=40033)\u001b[0m Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=40033)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=40033)\u001b[0m 2023-09-06 10:26:24.000337: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=40033)\u001b[0m Starting training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=40033)\u001b[0m You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=40033)\u001b[0m [W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=40033)\u001b[0m {'loss': 0.5464, 'learning_rate': 1e-05, 'epoch': 1.0}\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=40033)\u001b[0m {'eval_loss': 0.5178409218788147, 'eval_matthews_correlation': 0.40338161890982716, 'eval_runtime': 0.871, 'eval_samples_per_second': 1197.518, 'eval_steps_per_second': 75.778, 'epoch': 1.0}\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=40033)\u001b[0m {'loss': 0.3884, 'learning_rate': 0.0, 'epoch': 2.0}\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=40033)\u001b[0m {'eval_loss': 0.5500921607017517, 'eval_matthews_correlation': 0.45115517656589194, 'eval_runtime': 0.7425, 'eval_samples_per_second': 1404.739, 'eval_steps_per_second': 88.891, 'epoch': 2.0}\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=40033)\u001b[0m {'train_runtime': 89.372, 'train_samples_per_second': 191.357, 'train_steps_per_second': 11.972, 'train_loss': 0.46742234274605726, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-06 10:28:01,552\tINFO tune.py:1148 -- Total run time: 102.67 seconds (102.64 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "result = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Result(\n",
       "  metrics={'loss': 0.3884, 'learning_rate': 0.0, 'epoch': 2.0, 'step': 1070, 'eval_loss': 0.5500921607017517, 'eval_matthews_correlation': 0.45115517656589194, 'eval_runtime': 0.7425, 'eval_samples_per_second': 1404.739, 'eval_steps_per_second': 88.891, 'train_runtime': 89.372, 'train_samples_per_second': 191.357, 'train_steps_per_second': 11.972, 'train_loss': 0.46742234274605726, 'should_checkpoint': True, 'done': True, 'trial_id': '24dc3_00000', 'experiment_tag': '0'},\n",
       "  path='/home/mpp/ray_results/TransformersTrainer_2023-09-06_10-26-18/TransformersTrainer_24dc3_00000_0_2023-09-06_10-26-18',\n",
       "  checkpoint=TransformersCheckpoint(local_path=/home/mpp/ray_results/TransformersTrainer_2023-09-06_10-26-18/TransformersTrainer_24dc3_00000_0_2023-09-06_10-26-18/checkpoint_000001)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune hyperparameters with Ray AIR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-06 10:31:53,052\tINFO tuner_internal.py:490 -- A `RunConfig` was passed to both the `Tuner` and the `TransformersTrainer`. The run config passed to the `Tuner` is the one that will be used.\n"
     ]
    }
   ],
   "source": [
    "from ray import tune\n",
    "from ray.tune import Tuner\n",
    "from ray.tune.schedulers.async_hyperband import ASHAScheduler # to avoid the long wait, we use ASHA instead of Hyperband\n",
    "\n",
    "tune_epochs = 4\n",
    "tuner = Tuner(\n",
    "    trainer,\n",
    "    param_space={\n",
    "        \"trainer_init_config\": {\n",
    "            \"learning_rate\": tune.grid_search([2e-5, 2e-4, 2e-3, 2e-2]),\n",
    "            \"epochs\": tune_epochs,\n",
    "        }\n",
    "    },\n",
    "    tune_config=tune.TuneConfig(\n",
    "        metric=\"eval_loss\",\n",
    "        mode=\"min\",\n",
    "        num_samples=1,\n",
    "        scheduler=ASHAScheduler(\n",
    "            max_t=tune_epochs,\n",
    "        )\n",
    "    ),\n",
    "    run_config=RunConfig(\n",
    "        checkpoint_config=CheckpointConfig(num_to_keep=1, checkpoint_score_attribute=\"eval_loss\", checkpoint_score_order=\"min\")\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-09-06 10:37:17</td></tr>\n",
       "<tr><td>Running for: </td><td>00:05:05.69        </td></tr>\n",
       "<tr><td>Memory:      </td><td>21.5/30.9 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=4<br>Bracket: Iter 4.000: -0.7417590022087097 | Iter 1.000: -0.595201313495636<br>Logical resource usage: 1.0/20 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc                 </th><th style=\"text-align: right;\">       trainer_init_config/\n",
       "learning_rate</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  loss</th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">  epoch</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TransformersTrainer_f73ae_00000</td><td>TERMINATED</td><td>192.168.33.188:40811</td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        161.179 </td><td style=\"text-align: right;\">0.1952</td><td style=\"text-align: right;\">        0      </td><td style=\"text-align: right;\">      4</td></tr>\n",
       "<tr><td>TransformersTrainer_f73ae_00001</td><td>TERMINATED</td><td>192.168.33.188:41182</td><td style=\"text-align: right;\">0.0002</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         42.259 </td><td style=\"text-align: right;\">0.6276</td><td style=\"text-align: right;\">        0.00015</td><td style=\"text-align: right;\">      1</td></tr>\n",
       "<tr><td>TransformersTrainer_f73ae_00002</td><td>TERMINATED</td><td>192.168.33.188:41368</td><td style=\"text-align: right;\">0.002 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         43.7642</td><td style=\"text-align: right;\">0.645 </td><td style=\"text-align: right;\">        0.0015 </td><td style=\"text-align: right;\">      1</td></tr>\n",
       "<tr><td>TransformersTrainer_f73ae_00003</td><td>TERMINATED</td><td>192.168.33.188:41811</td><td style=\"text-align: right;\">0.02  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         41.6672</td><td style=\"text-align: right;\">1.0629</td><td style=\"text-align: right;\">        0.015  </td><td style=\"text-align: right;\">      1</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=40811)\u001b[0m 2023-09-06 10:32:13.563877: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[2m\u001b[36m(pid=40811)\u001b[0m 2023-09-06 10:32:13.589844: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[2m\u001b[36m(pid=40811)\u001b[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(pid=40811)\u001b[0m 2023-09-06 10:32:14.038696: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[2m\u001b[36m(TransformersTrainer pid=40811)\u001b[0m The `preprocessor` arg to Trainer is deprecated. Apply preprocessor transformations ahead of time by calling `preprocessor.transform(ds)`. Support for the preprocessor arg will be dropped in a future release.\n",
      "\u001b[2m\u001b[36m(TransformersTrainer pid=40811)\u001b[0m Starting distributed worker processes: ['40852 (192.168.33.188)']\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=40852)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=40852)\u001b[0m /home/mpp/.conda/envs/ray/lib/python3.9/site-packages/ray/train/huggingface/transformers/_transformers_utils.py:94: FutureWarning: 'format_type' is deprecated and will be removed in the next major version of datasets. Please use 'formatting=FormattingConfig(format_type=format_type)' instead.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=40852)\u001b[0m   iterable_dataset = datasets.iterable_dataset.IterableDataset(\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=40852)\u001b[0m Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[MapBatches(BatchMapper._transform_pandas)] -> AllToAllOperator[RandomizeBlockOrder]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=40852)\u001b[0m Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=40852)\u001b[0m Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9914f38b231f421a8708624ef23116ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=40852) - RandomizeBlockOrder 1:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5acb2aa2f5274c358d6a2410b3f83497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=40852) Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=40852)\u001b[0m Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[MapBatches(BatchMapper._transform_pandas)] -> AllToAllOperator[RandomizeBlockOrder]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=40852)\u001b[0m Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=40852)\u001b[0m Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=40852)\u001b[0m /tmp/ipykernel_39094/3612397856.py:4: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b02d03c649104967a3e4fe9526438b1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=40852) - RandomizeBlockOrder 1:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47eb8e1cb67d418881bc93df4205bbfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=40852) Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=40852)\u001b[0m Is CUDA available: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=40852)\u001b[0m Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=40852)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=40852)\u001b[0m 2023-09-06 10:32:16.567503: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=40852)\u001b[0m 2023-09-06 10:32:16.592535: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=40852)\u001b[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=40852)\u001b[0m 2023-09-06 10:32:17.034968: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=40852)\u001b[0m Starting training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=40852)\u001b[0m You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=40852)\u001b[0m [W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=40852)\u001b[0m {'loss': 0.543, 'learning_rate': 1.5000000000000002e-05, 'epoch': 1.0}\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=40852)\u001b[0m {'eval_loss': 0.5259996056556702, 'eval_matthews_correlation': 0.36986609141954047, 'eval_runtime': 0.8687, 'eval_samples_per_second': 1200.62, 'eval_steps_per_second': 75.974, 'epoch': 1.0}\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=40852)\u001b[0m {'loss': 0.3714, 'learning_rate': 1e-05, 'epoch': 2.0}\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=40852)\u001b[0m {'eval_loss': 0.5449074506759644, 'eval_matthews_correlation': 0.49016253095993895, 'eval_runtime': 0.7507, 'eval_samples_per_second': 1389.394, 'eval_steps_per_second': 87.919, 'epoch': 2.0}\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=40852)\u001b[0m {'loss': 0.254, 'learning_rate': 5e-06, 'epoch': 3.0}\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=40852)\u001b[0m {'eval_loss': 0.6437172293663025, 'eval_matthews_correlation': 0.5448598482839426, 'eval_runtime': 0.8837, 'eval_samples_per_second': 1180.284, 'eval_steps_per_second': 74.687, 'epoch': 3.0}\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=40852)\u001b[0m {'loss': 0.1952, 'learning_rate': 0.0, 'epoch': 4.0}\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=40852)\u001b[0m {'eval_loss': 0.7417590022087097, 'eval_matthews_correlation': 0.5181917740456299, 'eval_runtime': 0.7605, 'eval_samples_per_second': 1371.498, 'eval_steps_per_second': 86.787, 'epoch': 4.0}\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=40852)\u001b[0m {'train_runtime': 154.2166, 'train_samples_per_second': 221.792, 'train_steps_per_second': 13.877, 'train_loss': 0.34089398250401576, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=41182)\u001b[0m 2023-09-06 10:34:58.944375: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[2m\u001b[36m(pid=41182)\u001b[0m 2023-09-06 10:34:58.979472: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[2m\u001b[36m(pid=41182)\u001b[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(pid=41182)\u001b[0m 2023-09-06 10:34:59.570150: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[2m\u001b[36m(TransformersTrainer pid=41182)\u001b[0m The `preprocessor` arg to Trainer is deprecated. Apply preprocessor transformations ahead of time by calling `preprocessor.transform(ds)`. Support for the preprocessor arg will be dropped in a future release.\n",
      "\u001b[2m\u001b[36m(TransformersTrainer pid=41182)\u001b[0m Starting distributed worker processes: ['41235 (192.168.33.188)']\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41235)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41235)\u001b[0m /home/mpp/.conda/envs/ray/lib/python3.9/site-packages/ray/train/huggingface/transformers/_transformers_utils.py:94: FutureWarning: 'format_type' is deprecated and will be removed in the next major version of datasets. Please use 'formatting=FormattingConfig(format_type=format_type)' instead.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41235)\u001b[0m   iterable_dataset = datasets.iterable_dataset.IterableDataset(\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41235)\u001b[0m Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[MapBatches(BatchMapper._transform_pandas)] -> AllToAllOperator[RandomizeBlockOrder]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41235)\u001b[0m Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41235)\u001b[0m Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41235)\u001b[0m 2023-09-06 10:35:02.820011: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41235)\u001b[0m 2023-09-06 10:35:02.852320: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41235)\u001b[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96ab7d33ba1e4c3c8ff43a8cfb59ebbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=41235) - RandomizeBlockOrder 1:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3415cea68f704db08de90d5aa3a0e4bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=41235) Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41235)\u001b[0m Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[MapBatches(BatchMapper._transform_pandas)] -> AllToAllOperator[RandomizeBlockOrder]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41235)\u001b[0m Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41235)\u001b[0m Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41235)\u001b[0m /tmp/ipykernel_39094/3612397856.py:4: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b5040226b5145a9a4c81f4cd69891c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=41235) - RandomizeBlockOrder 1:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b5c9c5b070742cc9476b87b3df50373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=41235) Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41235)\u001b[0m Is CUDA available: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41235)\u001b[0m Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41235)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41235)\u001b[0m 2023-09-06 10:35:03.387473: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41235)\u001b[0m Starting training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41235)\u001b[0m You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41235)\u001b[0m [W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41235)\u001b[0m {'loss': 0.6276, 'learning_rate': 0.00015000000000000001, 'epoch': 1.0}\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41235)\u001b[0m {'eval_loss': 0.6183143854141235, 'eval_matthews_correlation': 0.0, 'eval_runtime': 0.7995, 'eval_samples_per_second': 1304.487, 'eval_steps_per_second': 82.547, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=41368)\u001b[0m 2023-09-06 10:35:45.990693: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[2m\u001b[36m(pid=41368)\u001b[0m 2023-09-06 10:35:46.025602: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[2m\u001b[36m(pid=41368)\u001b[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(pid=41368)\u001b[0m 2023-09-06 10:35:46.599202: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[2m\u001b[36m(TransformersTrainer pid=41368)\u001b[0m The `preprocessor` arg to Trainer is deprecated. Apply preprocessor transformations ahead of time by calling `preprocessor.transform(ds)`. Support for the preprocessor arg will be dropped in a future release.\n",
      "\u001b[2m\u001b[36m(TransformersTrainer pid=41368)\u001b[0m Starting distributed worker processes: ['41408 (192.168.33.188)']\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41408)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41408)\u001b[0m /home/mpp/.conda/envs/ray/lib/python3.9/site-packages/ray/train/huggingface/transformers/_transformers_utils.py:94: FutureWarning: 'format_type' is deprecated and will be removed in the next major version of datasets. Please use 'formatting=FormattingConfig(format_type=format_type)' instead.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41408)\u001b[0m   iterable_dataset = datasets.iterable_dataset.IterableDataset(\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41408)\u001b[0m Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[MapBatches(BatchMapper._transform_pandas)] -> AllToAllOperator[RandomizeBlockOrder]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41408)\u001b[0m Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41408)\u001b[0m Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41408)\u001b[0m 2023-09-06 10:35:49.949336: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41408)\u001b[0m 2023-09-06 10:35:49.986005: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41408)\u001b[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6baa797e26f40a0ae3391a11249ec1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=41408) - RandomizeBlockOrder 1:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "345d9b5d4e1c46e38c8a965d4f4435bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=41408) Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41408)\u001b[0m Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[MapBatches(BatchMapper._transform_pandas)] -> AllToAllOperator[RandomizeBlockOrder]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41408)\u001b[0m Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41408)\u001b[0m Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41408)\u001b[0m /tmp/ipykernel_39094/3612397856.py:4: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41408)\u001b[0m 2023-09-06 10:35:50.590174: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10bd96ac16a94ef686ab765128c9d86d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=41408) - RandomizeBlockOrder 1:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a18248c24c844da29babd3c10923b3e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=41408) Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41408)\u001b[0m Is CUDA available: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41408)\u001b[0m Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41408)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41408)\u001b[0m Starting training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41408)\u001b[0m You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41408)\u001b[0m [W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41408)\u001b[0m {'loss': 0.645, 'learning_rate': 0.0015, 'epoch': 1.0}\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41408)\u001b[0m {'eval_loss': 0.6195238828659058, 'eval_matthews_correlation': 0.0, 'eval_runtime': 0.7714, 'eval_samples_per_second': 1352.131, 'eval_steps_per_second': 85.562, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=41811)\u001b[0m 2023-09-06 10:36:34.134151: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[2m\u001b[36m(pid=41811)\u001b[0m 2023-09-06 10:36:34.174607: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[2m\u001b[36m(pid=41811)\u001b[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(pid=41811)\u001b[0m 2023-09-06 10:36:34.798906: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[2m\u001b[36m(TransformersTrainer pid=41811)\u001b[0m The `preprocessor` arg to Trainer is deprecated. Apply preprocessor transformations ahead of time by calling `preprocessor.transform(ds)`. Support for the preprocessor arg will be dropped in a future release.\n",
      "\u001b[2m\u001b[36m(TransformersTrainer pid=41811)\u001b[0m Starting distributed worker processes: ['41865 (192.168.33.188)']\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41865)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41865)\u001b[0m /home/mpp/.conda/envs/ray/lib/python3.9/site-packages/ray/train/huggingface/transformers/_transformers_utils.py:94: FutureWarning: 'format_type' is deprecated and will be removed in the next major version of datasets. Please use 'formatting=FormattingConfig(format_type=format_type)' instead.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41865)\u001b[0m   iterable_dataset = datasets.iterable_dataset.IterableDataset(\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41865)\u001b[0m Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[MapBatches(BatchMapper._transform_pandas)] -> AllToAllOperator[RandomizeBlockOrder]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41865)\u001b[0m Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41865)\u001b[0m Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41865)\u001b[0m 2023-09-06 10:36:38.180130: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41865)\u001b[0m 2023-09-06 10:36:38.217948: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41865)\u001b[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aa1dfd486c6405e895b12ec7469c9dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=41865) - RandomizeBlockOrder 1:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "389a8ce537744e29b888420177211f0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=41865) Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41865)\u001b[0m Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[MapBatches(BatchMapper._transform_pandas)] -> AllToAllOperator[RandomizeBlockOrder]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41865)\u001b[0m Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41865)\u001b[0m Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41865)\u001b[0m 2023-09-06 10:36:38.793213: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5d92e40a1664643ab8bb1b1ea9d1f10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=41865) - RandomizeBlockOrder 1:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c50f64d2d8ab448e86b96c40475bf59f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=41865) Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41865)\u001b[0m /tmp/ipykernel_39094/3612397856.py:4: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41865)\u001b[0m Is CUDA available: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41865)\u001b[0m Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41865)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41865)\u001b[0m Starting training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41865)\u001b[0m You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41865)\u001b[0m [W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41865)\u001b[0m {'loss': 1.0629, 'learning_rate': 0.015, 'epoch': 1.0}\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=41865)\u001b[0m {'eval_loss': 0.6182685494422913, 'eval_matthews_correlation': 0.0, 'eval_runtime': 0.7531, 'eval_samples_per_second': 1384.939, 'eval_steps_per_second': 87.638, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-06 10:37:17,524\tINFO tune.py:1148 -- Total run time: 305.70 seconds (305.69 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "tune_results = tuner.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_matthews_correlation</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>...</th>\n",
       "      <th>date</th>\n",
       "      <th>time_total_s</th>\n",
       "      <th>pid</th>\n",
       "      <th>hostname</th>\n",
       "      <th>node_ip</th>\n",
       "      <th>time_since_restore</th>\n",
       "      <th>iterations_since_restore</th>\n",
       "      <th>config/trainer_init_config/epochs</th>\n",
       "      <th>config/trainer_init_config/learning_rate</th>\n",
       "      <th>logdir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0629</td>\n",
       "      <td>0.01500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>535</td>\n",
       "      <td>0.618269</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.7531</td>\n",
       "      <td>1384.939</td>\n",
       "      <td>87.638</td>\n",
       "      <td>1693971437</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-09-06_10-37-17</td>\n",
       "      <td>41.667228</td>\n",
       "      <td>41811</td>\n",
       "      <td>fedora</td>\n",
       "      <td>192.168.33.188</td>\n",
       "      <td>41.667228</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.02000</td>\n",
       "      <td>/home/mpp/ray_results/TransformersTrainer_2023...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.6276</td>\n",
       "      <td>0.00015</td>\n",
       "      <td>1.0</td>\n",
       "      <td>535</td>\n",
       "      <td>0.618314</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.7995</td>\n",
       "      <td>1304.487</td>\n",
       "      <td>82.547</td>\n",
       "      <td>1693971342</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-09-06_10-35-42</td>\n",
       "      <td>42.259003</td>\n",
       "      <td>41182</td>\n",
       "      <td>fedora</td>\n",
       "      <td>192.168.33.188</td>\n",
       "      <td>42.259003</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00020</td>\n",
       "      <td>/home/mpp/ray_results/TransformersTrainer_2023...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.6450</td>\n",
       "      <td>0.00150</td>\n",
       "      <td>1.0</td>\n",
       "      <td>535</td>\n",
       "      <td>0.619524</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.7714</td>\n",
       "      <td>1352.131</td>\n",
       "      <td>85.562</td>\n",
       "      <td>1693971391</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-09-06_10-36-31</td>\n",
       "      <td>43.764205</td>\n",
       "      <td>41368</td>\n",
       "      <td>fedora</td>\n",
       "      <td>192.168.33.188</td>\n",
       "      <td>43.764205</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00200</td>\n",
       "      <td>/home/mpp/ray_results/TransformersTrainer_2023...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1952</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2140</td>\n",
       "      <td>0.741759</td>\n",
       "      <td>0.518192</td>\n",
       "      <td>0.7605</td>\n",
       "      <td>1371.498</td>\n",
       "      <td>86.787</td>\n",
       "      <td>1693971295</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-09-06_10-34-56</td>\n",
       "      <td>161.178861</td>\n",
       "      <td>40811</td>\n",
       "      <td>fedora</td>\n",
       "      <td>192.168.33.188</td>\n",
       "      <td>161.178861</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>/home/mpp/ray_results/TransformersTrainer_2023...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     loss  learning_rate  epoch  step  eval_loss  eval_matthews_correlation  \\\n",
       "3  1.0629        0.01500    1.0   535   0.618269                   0.000000   \n",
       "1  0.6276        0.00015    1.0   535   0.618314                   0.000000   \n",
       "2  0.6450        0.00150    1.0   535   0.619524                   0.000000   \n",
       "0  0.1952        0.00000    4.0  2140   0.741759                   0.518192   \n",
       "\n",
       "   eval_runtime  eval_samples_per_second  eval_steps_per_second   timestamp  \\\n",
       "3        0.7531                 1384.939                 87.638  1693971437   \n",
       "1        0.7995                 1304.487                 82.547  1693971342   \n",
       "2        0.7714                 1352.131                 85.562  1693971391   \n",
       "0        0.7605                 1371.498                 86.787  1693971295   \n",
       "\n",
       "   ...                 date  time_total_s    pid  hostname         node_ip  \\\n",
       "3  ...  2023-09-06_10-37-17     41.667228  41811    fedora  192.168.33.188   \n",
       "1  ...  2023-09-06_10-35-42     42.259003  41182    fedora  192.168.33.188   \n",
       "2  ...  2023-09-06_10-36-31     43.764205  41368    fedora  192.168.33.188   \n",
       "0  ...  2023-09-06_10-34-56    161.178861  40811    fedora  192.168.33.188   \n",
       "\n",
       "  time_since_restore  iterations_since_restore  \\\n",
       "3          41.667228                         1   \n",
       "1          42.259003                         1   \n",
       "2          43.764205                         1   \n",
       "0         161.178861                         4   \n",
       "\n",
       "   config/trainer_init_config/epochs config/trainer_init_config/learning_rate  \\\n",
       "3                                  4                                  0.02000   \n",
       "1                                  4                                  0.00020   \n",
       "2                                  4                                  0.00200   \n",
       "0                                  4                                  0.00002   \n",
       "\n",
       "                                              logdir  \n",
       "3  /home/mpp/ray_results/TransformersTrainer_2023...  \n",
       "1  /home/mpp/ray_results/TransformersTrainer_2023...  \n",
       "2  /home/mpp/ray_results/TransformersTrainer_2023...  \n",
       "0  /home/mpp/ray_results/TransformersTrainer_2023...  \n",
       "\n",
       "[4 rows x 25 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tune_results.get_dataframe().sort_values(\"eval_loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result = tune_results.get_best_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Result(\n",
       "  metrics={'loss': 1.0629, 'learning_rate': 0.015, 'epoch': 1.0, 'step': 535, 'eval_loss': 0.6182685494422913, 'eval_matthews_correlation': 0.0, 'eval_runtime': 0.7531, 'eval_samples_per_second': 1384.939, 'eval_steps_per_second': 87.638, 'should_checkpoint': True, 'done': True, 'trial_id': 'f73ae_00003', 'experiment_tag': '3_learning_rate=0.0200'},\n",
       "  path='/home/mpp/ray_results/TransformersTrainer_2023-09-06_10-31-53/TransformersTrainer_f73ae_00003_3_learning_rate=0.0200_2023-09-06_10-32-11',\n",
       "  checkpoint=TransformersCheckpoint(local_path=/home/mpp/ray_results/TransformersTrainer_2023-09-06_10-31-53/TransformersTrainer_f73ae_00003_3_learning_rate=0.0200_2023-09-06_10-32-11/checkpoint_000000)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, load the model and tokenizer locally, and recreate the ðŸ¤— Transformers Trainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.huggingface import TransformersCheckpoint\n",
    "\n",
    "checkpoint = TransformersCheckpoint.from_checkpoint(result.checkpoint)\n",
    "hf_trainer = checkpoint.get_model(model=AutoModelForSequenceClassification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'ray' from '/home/mpp/.conda/envs/ray/lib/python3.9/site-packages/ray/__init__.py'>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------ Shutdown Ray ------    \n",
    "ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ray",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
