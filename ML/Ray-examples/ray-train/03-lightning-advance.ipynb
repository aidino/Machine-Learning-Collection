{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune a BERT Text Classifier with LightningTrainer\n",
    "\n",
    "This is an advanced example for LightningTrainer, which demonstrates how to use LightningTrainer with Dataset.\n",
    "\n",
    "If you just want to quickly convert your existing PyTorch Lightning scripts into Ray AIR, you can refer to this starter example: Train a Pytorch Lightning Image Classifier.\n",
    "\n",
    "Source: https://docs.ray.io/en/latest/train/examples/lightning/lightning_cola_advanced.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset, load_metric\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process CoLA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-07 17:06:50,349\tWARNING read_api.py:1794 -- You provided a Huggingface DatasetDict which contains multiple datasets. The output of `from_huggingface` is a dictionary of Ray Datasets. To convert just a single Huggingface Dataset to a Ray Dataset, specify a split. For example, `ray.data.from_huggingface(my_dataset_dictionary['train'])`. Available splits are ['train', 'validation', 'test'].\n",
      "2023-09-07 17:06:53,841\tINFO worker.py:1612 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"glue\", \"cola\")\n",
    "metric = load_metric(\"glue\", \"cola\")\n",
    "\n",
    "ray_datasets = ray.data.from_huggingface(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (â€¦)okenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29.0/29.0 [00:00<00:00, 3.10kB/s]\n",
      "Downloading (â€¦)lve/main/config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 570/570 [00:00<00:00, 59.8kB/s]\n",
      "Downloading (â€¦)solve/main/vocab.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 213k/213k [00:00<00:00, 395kB/s]\n",
      "Downloading (â€¦)/main/tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 436k/436k [00:11<00:00, 38.2kB/s]\n"
     ]
    }
   ],
   "source": [
    "from ray.data.preprocessors import BatchMapper\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "\n",
    "def tokenize_sentence(batch):\n",
    "    encoded_sent = tokenizer(\n",
    "        batch[\"sentence\"].tolist(),\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    batch[\"input_ids\"] = encoded_sent[\"input_ids\"].numpy()\n",
    "    batch[\"attention_mask\"] = encoded_sent[\"attention_mask\"].numpy()\n",
    "    batch[\"label\"] = np.array(batch[\"label\"])\n",
    "    batch.pop(\"sentence\")\n",
    "    return batch\n",
    "\n",
    "\n",
    "preprocessor = BatchMapper(tokenize_sentence, batch_format=\"numpy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a PyTorch Lightning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentModel(pl.LightningModule):\n",
    "    def __init__(self, lr=2e-5, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.eps = eps # epsilon\n",
    "        self.num_classes = 2\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            \"bert-base-cased\", num_labels=self.num_classes\n",
    "        )\n",
    "        self.metric = load_metric(\"glue\", \"cola\")\n",
    "        self.predictions = []\n",
    "        self.references = []\n",
    "\n",
    "    def forward(self, batch):\n",
    "        input_ids, attention_mask = batch[\"input_ids\"], batch[\"attention_mask\"]\n",
    "        outputs = self.model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        labels = batch[\"label\"]\n",
    "        logits = self.forward(batch)\n",
    "        loss = F.cross_entropy(logits.view(-1, self.num_classes), labels)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        labels = batch[\"label\"]\n",
    "        logits = self.forward(batch)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.predictions.append(preds)\n",
    "        self.references.append(labels)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        predictions = torch.concat(self.predictions).view(-1)\n",
    "        references = torch.concat(self.references).view(-1)\n",
    "        matthews_correlation = self.metric.compute(\n",
    "            predictions=predictions, references=references\n",
    "        )\n",
    "\n",
    "        # self.metric.compute() returns a dictionary:\n",
    "        # e.g. {\"matthews_correlation\": 0.53}\n",
    "        self.log_dict(matthews_correlation, sync_dist=True)\n",
    "        self.predictions.clear()\n",
    "        self.references.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.lr, eps=self.eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure your LightningTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.lightning import LightningTrainer, LightningConfigBuilder\n",
    "from ray.air.config import RunConfig, ScalingConfig, CheckpointConfig\n",
    "\n",
    "# Define the configs for LightningTrainer\n",
    "lightning_config = (\n",
    "    LightningConfigBuilder()\n",
    "    .module(cls=SentimentModel, lr=1e-5, eps=1e-8)\n",
    "    .trainer(max_epochs=5, accelerator=\"gpu\")\n",
    "    .checkpointing(save_on_train_epoch_end=False)\n",
    "    .build()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save AIR checkpoints according to the performance on validation set\n",
    "run_config = RunConfig(\n",
    "    name=\"ptl-sent-classification\",\n",
    "    checkpoint_config=CheckpointConfig(\n",
    "        num_to_keep=2,\n",
    "        checkpoint_score_attribute=\"matthews_correlation\",\n",
    "        checkpoint_score_order=\"max\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Scale the DDP training workload across 4 GPUs\n",
    "# You can change this config based on your compute resources.\n",
    "scaling_config = ScalingConfig(\n",
    "    num_workers=1, use_gpu=True, resources_per_worker={\"CPU\": 15, \"GPU\": 1}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune the model with LightningTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-09-07 17:38:50</td></tr>\n",
       "<tr><td>Running for: </td><td>00:19:03.66        </td></tr>\n",
       "<tr><td>Memory:      </td><td>8.1/30.9 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 16.0/20 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                 </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>LightningTrainer_11eaa_00000</td><td>RUNNING </td><td>192.168.33.188:12028</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(LightningTrainer pid=12028)\u001b[0m The `preprocessor` arg to Trainer is deprecated. Apply preprocessor transformations ahead of time by calling `preprocessor.transform(ds)`. Support for the preprocessor arg will be dropped in a future release.\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=12028)\u001b[0m Starting distributed worker processes: ['12071 (192.168.33.188)']\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=12071)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "Downloading model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]\n",
      "Downloading model.safetensors:   2%|â–         | 10.5M/436M [00:44<29:50, 238kB/s]\n",
      "Downloading model.safetensors:   2%|â–         | 10.5M/436M [01:00<29:50, 238kB/s]\n",
      "Downloading model.safetensors:   5%|â–         | 21.0M/436M [01:01<18:42, 369kB/s]\n",
      "Downloading model.safetensors:   5%|â–         | 21.0M/436M [01:20<18:42, 369kB/s]\n",
      "Downloading model.safetensors:   7%|â–‹         | 31.5M/436M [01:20<15:31, 434kB/s]\n",
      "Downloading model.safetensors:   7%|â–‹         | 31.5M/436M [01:40<15:31, 434kB/s]\n",
      "Downloading model.safetensors:  10%|â–‰         | 41.9M/436M [01:50<16:33, 396kB/s]\n",
      "Downloading model.safetensors:  10%|â–‰         | 41.9M/436M [02:10<16:33, 396kB/s]\n",
      "Downloading model.safetensors:  12%|â–ˆâ–        | 52.4M/436M [02:15<15:43, 406kB/s]\n",
      "Downloading model.safetensors:  12%|â–ˆâ–        | 52.4M/436M [02:30<15:43, 406kB/s]\n",
      "Downloading model.safetensors:  14%|â–ˆâ–        | 62.9M/436M [02:48<16:41, 372kB/s]\n",
      "Downloading model.safetensors:  14%|â–ˆâ–        | 62.9M/436M [03:00<16:41, 372kB/s]\n",
      "Downloading model.safetensors:  17%|â–ˆâ–‹        | 73.4M/436M [03:09<14:54, 405kB/s]\n",
      "Downloading model.safetensors:  17%|â–ˆâ–‹        | 73.4M/436M [03:20<14:54, 405kB/s]\n",
      "Downloading model.safetensors:  19%|â–ˆâ–‰        | 83.9M/436M [03:27<13:06, 448kB/s]\n",
      "Downloading model.safetensors:  19%|â–ˆâ–‰        | 83.9M/436M [03:40<13:06, 448kB/s]\n",
      "Downloading model.safetensors:  22%|â–ˆâ–ˆâ–       | 94.4M/436M [03:44<11:41, 487kB/s]\n",
      "Downloading model.safetensors:  22%|â–ˆâ–ˆâ–       | 94.4M/436M [04:00<11:41, 487kB/s]\n",
      "Downloading model.safetensors:  24%|â–ˆâ–ˆâ–       | 105M/436M [04:02<10:39, 517kB/s] \n",
      "Downloading model.safetensors:  24%|â–ˆâ–ˆâ–       | 105M/436M [04:20<10:39, 517kB/s]\n",
      "Downloading model.safetensors:  26%|â–ˆâ–ˆâ–‹       | 115M/436M [04:37<12:36, 424kB/s]\n",
      "Downloading model.safetensors:  26%|â–ˆâ–ˆâ–‹       | 115M/436M [04:50<12:36, 424kB/s]\n",
      "Downloading model.safetensors:  29%|â–ˆâ–ˆâ–‰       | 126M/436M [04:55<11:12, 461kB/s]\n",
      "Downloading model.safetensors:  29%|â–ˆâ–ˆâ–‰       | 126M/436M [05:10<11:12, 461kB/s]\n",
      "Downloading model.safetensors:  31%|â–ˆâ–ˆâ–ˆâ–      | 136M/436M [05:16<10:34, 472kB/s]\n",
      "Downloading model.safetensors:  31%|â–ˆâ–ˆâ–ˆâ–      | 136M/436M [05:30<10:34, 472kB/s]\n",
      "Downloading model.safetensors:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 147M/436M [05:45<11:08, 432kB/s]\n",
      "Downloading model.safetensors:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 147M/436M [06:00<11:08, 432kB/s]\n",
      "Downloading model.safetensors:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 157M/436M [06:08<10:32, 440kB/s]\n",
      "Downloading model.safetensors:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 157M/436M [06:20<10:32, 440kB/s]\n",
      "Downloading model.safetensors:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 168M/436M [06:36<10:47, 414kB/s]\n",
      "Downloading model.safetensors:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 168M/436M [06:50<10:47, 414kB/s]\n",
      "Downloading model.safetensors:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178M/436M [07:02<10:27, 410kB/s]\n",
      "Downloading model.safetensors:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178M/436M [07:20<10:27, 410kB/s]\n",
      "Downloading model.safetensors:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 189M/436M [07:21<09:11, 448kB/s]\n",
      "Downloading model.safetensors:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 189M/436M [07:40<09:11, 448kB/s]\n",
      "Downloading model.safetensors:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 199M/436M [07:47<09:06, 432kB/s]\n",
      "Downloading model.safetensors:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 199M/436M [08:00<09:06, 432kB/s]\n",
      "Downloading model.safetensors:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 210M/436M [08:05<07:59, 471kB/s]\n",
      "Downloading model.safetensors:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 210M/436M [08:20<07:59, 471kB/s]\n",
      "Downloading model.safetensors:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 220M/436M [08:22<07:06, 505kB/s]\n",
      "Downloading model.safetensors:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 220M/436M [08:40<07:06, 505kB/s]\n",
      "Downloading model.safetensors:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 231M/436M [08:42<06:39, 514kB/s]\n",
      "Downloading model.safetensors:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 241M/436M [08:59<06:04, 533kB/s]\n",
      "Downloading model.safetensors:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 241M/436M [09:10<06:04, 533kB/s]\n",
      "Downloading model.safetensors:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 252M/436M [09:18<05:37, 545kB/s]\n",
      "Downloading model.safetensors:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 252M/436M [09:30<05:37, 545kB/s]\n",
      "Downloading model.safetensors:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 262M/436M [09:53<06:38, 435kB/s]\n",
      "Downloading model.safetensors:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 262M/436M [10:10<06:38, 435kB/s]\n",
      "Downloading model.safetensors:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273M/436M [10:11<05:43, 475kB/s]\n",
      "Downloading model.safetensors:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 283M/436M [10:28<05:01, 507kB/s]\n",
      "Downloading model.safetensors:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 283M/436M [10:40<05:01, 507kB/s]\n",
      "Downloading model.safetensors:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 294M/436M [11:10<06:07, 387kB/s]\n",
      "Downloading model.safetensors:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 294M/436M [11:30<06:07, 387kB/s]\n",
      "Downloading model.safetensors:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 304M/436M [11:32<05:20, 410kB/s]\n",
      "Downloading model.safetensors:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 315M/436M [11:49<04:26, 454kB/s]\n",
      "Downloading model.safetensors:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 315M/436M [12:00<04:26, 454kB/s]\n",
      "Downloading model.safetensors:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 325M/436M [12:16<04:16, 432kB/s]\n",
      "Downloading model.safetensors:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 325M/436M [12:30<04:16, 432kB/s]\n",
      "Downloading model.safetensors:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 336M/436M [12:34<03:32, 472kB/s]\n",
      "Downloading model.safetensors:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 336M/436M [12:50<03:32, 472kB/s]\n",
      "Downloading model.safetensors:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 346M/436M [12:53<03:03, 489kB/s]\n",
      "Downloading model.safetensors:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 346M/436M [13:10<03:03, 489kB/s]\n",
      "Downloading model.safetensors:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 357M/436M [13:13<02:38, 501kB/s]\n",
      "Downloading model.safetensors:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 357M/436M [13:30<02:38, 501kB/s]\n",
      "Downloading model.safetensors:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 367M/436M [13:32<02:12, 520kB/s]\n",
      "Downloading model.safetensors:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 377M/436M [13:49<01:47, 542kB/s]\n",
      "Downloading model.safetensors:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 377M/436M [14:00<01:47, 542kB/s]\n",
      "Downloading model.safetensors:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388M/436M [14:07<01:26, 555kB/s]\n",
      "Downloading model.safetensors:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388M/436M [14:20<01:26, 555kB/s]\n",
      "Downloading model.safetensors:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398M/436M [14:27<01:08, 542kB/s]\n",
      "Downloading model.safetensors:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398M/436M [14:40<01:08, 542kB/s]\n",
      "Downloading model.safetensors:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 409M/436M [14:46<00:48, 551kB/s]\n",
      "Downloading model.safetensors:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 409M/436M [15:00<00:48, 551kB/s]\n",
      "Downloading model.safetensors:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 419M/436M [15:03<00:28, 564kB/s]\n",
      "Downloading model.safetensors:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 419M/436M [15:20<00:28, 564kB/s]\n",
      "Downloading model.safetensors:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 430M/436M [15:21<00:10, 567kB/s]\n",
      "Downloading model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 436M/436M [15:32<00:00, 467kB/s]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=12071)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=12071)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=12071)\u001b[0m /tmp/ipykernel_8468/3457447719.py:10: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=12071)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=12071)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=12071)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=12071)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=12071)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 4070 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=12071)\u001b[0m Missing logger folder: /home/mpp/ray_results/ptl-sent-classification/LightningTrainer_11eaa_00000_0_2023-09-07_17-19-46/rank_all/lightning_logs\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=12071)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=12071)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=12071)\u001b[0m   | Name  | Type                          | Params\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=12071)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=12071)\u001b[0m 0 | model | BertForSequenceClassification | 108 M \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=12071)\u001b[0m --------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=12071)\u001b[0m 108 M     Trainable params\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=12071)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=12071)\u001b[0m 108 M     Total params\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=12071)\u001b[0m 433.247   Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=12071)\u001b[0m /home/mpp/miniconda3/envs/ray-torch/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=12071)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=12071)\u001b[0m Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[MapBatches(BatchMapper._transform_numpy)] -> AllToAllOperator[RandomizeBlockOrder]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=12071)\u001b[0m Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=12071)\u001b[0m Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n",
      "Sanity Checking: 0it [00:00, ?it/s]{\"__magic_token__\": \"__ray_tqdm_magic_token__\", \"x\": 0, \"pos\": 1, \"desc\": \"- RandomizeBlockOrder\", \"total\": 1, \"ip\": \"192.168.33.188\", \"pid\": 12071, \"uuid\": \"c04daa520e8344f09666d1f99319fdf9\", \"closed\": false}, this may be due to logging too fast. This warning will not be printed again.\n",
      "                                                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Sanity Checking DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  2.60it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=12071)\u001b[0m /home/mpp/miniconda3/envs/ray-torch/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=12071)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=12071)\u001b[0m Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[MapBatches(BatchMapper._transform_numpy)] -> AllToAllOperator[RandomizeBlockOrder]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=12071)\u001b[0m Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=12071)\u001b[0m Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n",
      "                                                                                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: : 1it [00:00,  1.07it/s, v_num=0]\n",
      "Epoch 0: : 2it [00:01,  1.36it/s, v_num=0]\n",
      "Epoch 0: : 3it [00:02,  1.48it/s, v_num=0]\n",
      "Epoch 0: : 4it [00:02,  1.57it/s, v_num=0]\n",
      "Epoch 0: : 5it [00:03,  1.61it/s, v_num=0]\n",
      "Epoch 0: : 6it [00:03,  1.64it/s, v_num=0]\n",
      "Epoch 0: : 7it [00:04,  1.66it/s, v_num=0]\n",
      "Epoch 0: : 8it [00:04,  1.68it/s, v_num=0]\n",
      "Epoch 0: : 9it [00:05,  1.69it/s, v_num=0]\n",
      "Epoch 0: : 10it [00:05,  1.70it/s, v_num=0]\n",
      "Epoch 0: : 11it [00:06,  1.71it/s, v_num=0]\n",
      "Epoch 0: : 12it [00:06,  1.72it/s, v_num=0]\n",
      "Epoch 0: : 13it [00:07,  1.73it/s, v_num=0]\n",
      "Epoch 0: : 14it [00:08,  1.73it/s, v_num=0]\n",
      "Epoch 0: : 15it [00:08,  1.73it/s, v_num=0]\n",
      "Epoch 0: : 16it [00:09,  1.74it/s, v_num=0]\n",
      "Epoch 0: : 17it [00:09,  1.74it/s, v_num=0]\n",
      "Epoch 0: : 18it [00:10,  1.74it/s, v_num=0]\n",
      "Epoch 0: : 19it [00:10,  1.75it/s, v_num=0]\n",
      "Epoch 0: : 20it [00:11,  1.75it/s, v_num=0]\n",
      "Epoch 0: : 21it [00:11,  1.75it/s, v_num=0]\n",
      "Epoch 0: : 22it [00:12,  1.75it/s, v_num=0]\n",
      "Epoch 0: : 23it [00:13,  1.75it/s, v_num=0]\n",
      "Epoch 0: : 24it [00:13,  1.76it/s, v_num=0]\n",
      "Epoch 0: : 25it [00:14,  1.76it/s, v_num=0]\n",
      "Epoch 0: : 26it [00:14,  1.76it/s, v_num=0]\n",
      "Epoch 0: : 27it [00:15,  1.76it/s, v_num=0]\n",
      "Epoch 0: : 28it [00:15,  1.76it/s, v_num=0]\n",
      "Epoch 0: : 29it [00:16,  1.76it/s, v_num=0]\n",
      "Epoch 0: : 30it [00:17,  1.76it/s, v_num=0]\n",
      "Epoch 0: : 31it [00:17,  1.76it/s, v_num=0]\n",
      "Epoch 0: : 32it [00:18,  1.76it/s, v_num=0]\n",
      "Epoch 0: : 33it [00:18,  1.76it/s, v_num=0]\n",
      "Epoch 0: : 34it [00:19,  1.77it/s, v_num=0]\n",
      "Epoch 0: : 35it [00:20,  1.73it/s, v_num=0]\n",
      "Epoch 0: : 36it [00:21,  1.70it/s, v_num=0]\n",
      "Epoch 0: : 37it [00:21,  1.70it/s, v_num=0]\n",
      "Epoch 0: : 38it [00:22,  1.71it/s, v_num=0]\n",
      "Epoch 0: : 39it [00:23,  1.68it/s, v_num=0]\n",
      "Epoch 0: : 40it [00:24,  1.66it/s, v_num=0]\n",
      "Epoch 0: : 41it [00:25,  1.64it/s, v_num=0]\n",
      "Epoch 0: : 42it [00:25,  1.62it/s, v_num=0]\n",
      "Epoch 0: : 43it [00:26,  1.62it/s, v_num=0]\n",
      "Epoch 0: : 44it [00:27,  1.63it/s, v_num=0]\n",
      "Epoch 0: : 45it [00:27,  1.61it/s, v_num=0]\n",
      "Epoch 0: : 46it [00:28,  1.59it/s, v_num=0]\n",
      "Epoch 0: : 47it [00:29,  1.59it/s, v_num=0]\n",
      "Epoch 0: : 48it [00:30,  1.57it/s, v_num=0]\n",
      "Epoch 0: : 49it [00:31,  1.56it/s, v_num=0]\n",
      "Epoch 0: : 50it [00:31,  1.57it/s, v_num=0]\n",
      "Epoch 0: : 51it [00:32,  1.57it/s, v_num=0]\n",
      "Epoch 0: : 52it [00:33,  1.56it/s, v_num=0]\n",
      "Epoch 0: : 53it [00:34,  1.55it/s, v_num=0]\n",
      "Epoch 0: : 54it [00:35,  1.53it/s, v_num=0]\n",
      "Epoch 0: : 55it [00:36,  1.52it/s, v_num=0]\n",
      "Epoch 0: : 56it [00:37,  1.51it/s, v_num=0]\n",
      "Epoch 0: : 57it [00:37,  1.50it/s, v_num=0]\n",
      "Epoch 0: : 58it [00:38,  1.49it/s, v_num=0]\n",
      "Epoch 0: : 59it [00:39,  1.49it/s, v_num=0]\n",
      "Epoch 0: : 60it [00:40,  1.49it/s, v_num=0]\n",
      "Epoch 0: : 61it [00:41,  1.48it/s, v_num=0]\n",
      "Epoch 0: : 62it [00:42,  1.47it/s, v_num=0]\n",
      "Epoch 0: : 63it [00:43,  1.46it/s, v_num=0]\n",
      "Epoch 0: : 64it [00:43,  1.46it/s, v_num=0]\n",
      "Epoch 0: : 65it [00:44,  1.45it/s, v_num=0]\n",
      "Epoch 0: : 66it [00:45,  1.44it/s, v_num=0]\n",
      "Epoch 0: : 67it [00:46,  1.44it/s, v_num=0]\n",
      "Epoch 0: : 68it [00:47,  1.43it/s, v_num=0]\n",
      "Epoch 0: : 69it [00:48,  1.44it/s, v_num=0]\n",
      "Epoch 0: : 70it [00:48,  1.43it/s, v_num=0]\n",
      "Epoch 0: : 71it [00:49,  1.42it/s, v_num=0]\n",
      "Epoch 0: : 72it [00:50,  1.43it/s, v_num=0]\n",
      "Epoch 0: : 73it [00:51,  1.42it/s, v_num=0]\n",
      "Epoch 0: : 74it [00:52,  1.41it/s, v_num=0]\n",
      "Epoch 0: : 75it [00:52,  1.42it/s, v_num=0]\n",
      "Epoch 0: : 76it [00:53,  1.42it/s, v_num=0]\n",
      "Epoch 0: : 77it [00:54,  1.43it/s, v_num=0]\n",
      "Epoch 0: : 78it [00:54,  1.43it/s, v_num=0]\n",
      "Epoch 0: : 79it [00:55,  1.43it/s, v_num=0]\n",
      "Epoch 0: : 80it [00:55,  1.44it/s, v_num=0]\n",
      "Epoch 0: : 81it [00:56,  1.44it/s, v_num=0]\n",
      "Epoch 0: : 82it [00:56,  1.44it/s, v_num=0]\n",
      "Epoch 0: : 83it [00:57,  1.44it/s, v_num=0]\n",
      "Epoch 0: : 84it [00:58,  1.43it/s, v_num=0]\n",
      "Epoch 0: : 85it [00:59,  1.43it/s, v_num=0]\n",
      "Epoch 0: : 86it [01:00,  1.42it/s, v_num=0]\n",
      "Epoch 0: : 87it [01:01,  1.42it/s, v_num=0]\n",
      "Epoch 0: : 88it [01:02,  1.41it/s, v_num=0]\n",
      "Epoch 0: : 89it [01:03,  1.41it/s, v_num=0]\n",
      "Epoch 0: : 90it [01:04,  1.40it/s, v_num=0]\n",
      "Epoch 0: : 91it [01:05,  1.40it/s, v_num=0]\n",
      "Epoch 0: : 92it [01:05,  1.40it/s, v_num=0]\n",
      "Epoch 0: : 93it [01:06,  1.40it/s, v_num=0]\n",
      "Epoch 0: : 94it [01:07,  1.40it/s, v_num=0]\n",
      "Epoch 0: : 95it [01:08,  1.40it/s, v_num=0]\n",
      "Epoch 0: : 96it [01:08,  1.39it/s, v_num=0]\n",
      "Epoch 0: : 97it [01:09,  1.39it/s, v_num=0]\n",
      "Epoch 0: : 98it [01:10,  1.39it/s, v_num=0]\n",
      "Epoch 0: : 99it [01:11,  1.39it/s, v_num=0]\n",
      "Epoch 0: : 100it [01:12,  1.38it/s, v_num=0]\n",
      "Epoch 0: : 101it [01:13,  1.38it/s, v_num=0]\n",
      "Epoch 0: : 102it [01:13,  1.38it/s, v_num=0]\n",
      "Epoch 0: : 103it [01:14,  1.39it/s, v_num=0]\n",
      "Epoch 0: : 104it [01:15,  1.38it/s, v_num=0]\n",
      "Epoch 0: : 105it [01:16,  1.38it/s, v_num=0]\n",
      "Epoch 0: : 106it [01:17,  1.38it/s, v_num=0]\n",
      "Epoch 0: : 107it [01:17,  1.37it/s, v_num=0]\n",
      "Epoch 0: : 108it [01:18,  1.38it/s, v_num=0]\n",
      "Epoch 0: : 109it [01:19,  1.37it/s, v_num=0]\n",
      "Epoch 0: : 110it [01:20,  1.37it/s, v_num=0]\n",
      "Epoch 0: : 111it [01:21,  1.37it/s, v_num=0]\n",
      "Epoch 0: : 112it [01:22,  1.36it/s, v_num=0]\n",
      "Epoch 0: : 113it [01:22,  1.36it/s, v_num=0]\n",
      "Epoch 0: : 114it [01:23,  1.36it/s, v_num=0]\n",
      "Epoch 0: : 115it [01:24,  1.36it/s, v_num=0]\n",
      "Epoch 0: : 116it [01:25,  1.36it/s, v_num=0]\n",
      "Epoch 0: : 117it [01:26,  1.36it/s, v_num=0]\n",
      "Epoch 0: : 118it [01:26,  1.36it/s, v_num=0]\n",
      "Epoch 0: : 119it [01:27,  1.35it/s, v_num=0]\n",
      "Epoch 0: : 120it [01:28,  1.35it/s, v_num=0]\n",
      "Epoch 0: : 121it [01:29,  1.36it/s, v_num=0]\n",
      "Epoch 0: : 122it [01:30,  1.35it/s, v_num=0]\n",
      "Epoch 0: : 122it [01:30,  1.35it/s, v_num=0]\n",
      "Epoch 0: : 123it [01:31,  1.35it/s, v_num=0]\n",
      "Epoch 0: : 124it [01:32,  1.35it/s, v_num=0]\n",
      "Epoch 0: : 125it [01:32,  1.35it/s, v_num=0]\n",
      "Epoch 0: : 126it [01:33,  1.35it/s, v_num=0]\n",
      "Epoch 0: : 127it [01:34,  1.34it/s, v_num=0]\n",
      "Epoch 0: : 128it [01:35,  1.34it/s, v_num=0]\n",
      "Epoch 0: : 129it [01:36,  1.34it/s, v_num=0]\n",
      "Epoch 0: : 130it [01:37,  1.34it/s, v_num=0]\n",
      "Epoch 0: : 131it [01:38,  1.34it/s, v_num=0]\n",
      "Epoch 0: : 132it [01:38,  1.33it/s, v_num=0]\n",
      "Epoch 0: : 133it [01:39,  1.33it/s, v_num=0]\n",
      "Epoch 0: : 134it [01:40,  1.33it/s, v_num=0]\n",
      "Epoch 0: : 135it [01:41,  1.33it/s, v_num=0]\n",
      "Epoch 0: : 136it [01:42,  1.33it/s, v_num=0]\n",
      "Epoch 0: : 137it [01:43,  1.32it/s, v_num=0]\n",
      "Epoch 0: : 138it [01:44,  1.32it/s, v_num=0]\n",
      "Epoch 0: : 139it [01:44,  1.33it/s, v_num=0]\n",
      "Epoch 0: : 140it [01:45,  1.33it/s, v_num=0]\n",
      "Epoch 0: : 141it [01:46,  1.33it/s, v_num=0]\n",
      "Epoch 0: : 142it [01:47,  1.32it/s, v_num=0]\n",
      "Epoch 0: : 143it [01:48,  1.32it/s, v_num=0]\n",
      "Epoch 0: : 144it [01:49,  1.32it/s, v_num=0]\n",
      "Epoch 0: : 145it [01:50,  1.32it/s, v_num=0]\n",
      "Epoch 0: : 146it [01:50,  1.32it/s, v_num=0]\n",
      "Epoch 0: : 147it [01:51,  1.31it/s, v_num=0]\n",
      "Epoch 0: : 148it [01:52,  1.31it/s, v_num=0]\n",
      "Epoch 0: : 149it [01:53,  1.31it/s, v_num=0]\n",
      "Epoch 0: : 150it [01:54,  1.31it/s, v_num=0]\n",
      "Epoch 0: : 151it [01:55,  1.31it/s, v_num=0]\n",
      "Epoch 0: : 152it [01:56,  1.31it/s, v_num=0]\n",
      "Epoch 0: : 153it [01:57,  1.31it/s, v_num=0]\n",
      "Epoch 0: : 154it [01:58,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 155it [01:59,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 156it [01:59,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 157it [02:00,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 158it [02:01,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 159it [02:02,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 160it [02:03,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 161it [02:04,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 162it [02:04,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 163it [02:05,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 164it [02:06,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 165it [02:07,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 166it [02:07,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 167it [02:08,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 168it [02:09,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 169it [02:10,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 170it [02:10,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 171it [02:11,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 172it [02:11,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 173it [02:12,  1.31it/s, v_num=0]\n",
      "Epoch 0: : 174it [02:13,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 175it [02:14,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 176it [02:14,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 177it [02:15,  1.31it/s, v_num=0]\n",
      "Epoch 0: : 178it [02:16,  1.31it/s, v_num=0]\n",
      "Epoch 0: : 179it [02:17,  1.31it/s, v_num=0]\n",
      "Epoch 0: : 180it [02:17,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 181it [02:18,  1.31it/s, v_num=0]\n",
      "Epoch 0: : 182it [02:19,  1.31it/s, v_num=0]\n",
      "Epoch 0: : 183it [02:19,  1.31it/s, v_num=0]\n",
      "Epoch 0: : 184it [02:20,  1.31it/s, v_num=0]\n",
      "Epoch 0: : 185it [02:21,  1.31it/s, v_num=0]\n",
      "Epoch 0: : 186it [02:22,  1.31it/s, v_num=0]\n",
      "Epoch 0: : 187it [02:22,  1.31it/s, v_num=0]\n",
      "Epoch 0: : 188it [02:23,  1.31it/s, v_num=0]\n",
      "Epoch 0: : 189it [02:24,  1.31it/s, v_num=0]\n",
      "Epoch 0: : 190it [02:25,  1.31it/s, v_num=0]\n",
      "Epoch 0: : 191it [02:26,  1.31it/s, v_num=0]\n",
      "Epoch 0: : 192it [02:26,  1.31it/s, v_num=0]\n",
      "Epoch 0: : 193it [02:27,  1.31it/s, v_num=0]\n",
      "Epoch 0: : 194it [02:28,  1.31it/s, v_num=0]\n",
      "Epoch 0: : 195it [02:29,  1.31it/s, v_num=0]\n",
      "Epoch 0: : 196it [02:29,  1.31it/s, v_num=0]\n",
      "Epoch 0: : 197it [02:30,  1.31it/s, v_num=0]\n",
      "Epoch 0: : 198it [02:31,  1.31it/s, v_num=0]\n",
      "Epoch 0: : 199it [02:32,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 200it [02:33,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 201it [02:34,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 202it [02:35,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 203it [02:35,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 204it [02:36,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 205it [02:37,  1.31it/s, v_num=0]\n",
      "Epoch 0: : 206it [02:38,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 207it [02:38,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 208it [02:39,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 209it [02:40,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 210it [02:41,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 211it [02:41,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 212it [02:42,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 213it [02:43,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 214it [02:44,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 215it [02:44,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 216it [02:45,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 217it [02:46,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 218it [02:47,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 219it [02:48,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 220it [02:49,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 221it [02:50,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 222it [02:51,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 223it [02:52,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 224it [02:52,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 225it [02:53,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 226it [02:53,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 227it [02:54,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 228it [02:55,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 229it [02:56,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 230it [02:56,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 231it [02:57,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 232it [02:58,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 233it [02:59,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 234it [02:59,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 235it [03:00,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 236it [03:01,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 237it [03:02,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 238it [03:03,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 239it [03:04,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 240it [03:05,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 241it [03:05,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 242it [03:06,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 243it [03:07,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 244it [03:08,  1.30it/s, v_num=0]\n",
      "Epoch 0: : 245it [03:09,  1.29it/s, v_num=0]\n",
      "Epoch 0: : 246it [03:10,  1.29it/s, v_num=0]\n",
      "Epoch 0: : 247it [03:11,  1.29it/s, v_num=0]\n",
      "Epoch 0: : 248it [03:11,  1.29it/s, v_num=0]\n",
      "Epoch 0: : 249it [03:12,  1.29it/s, v_num=0]\n",
      "Epoch 0: : 250it [03:13,  1.29it/s, v_num=0]\n",
      "Epoch 0: : 251it [03:14,  1.29it/s, v_num=0]\n",
      "Epoch 0: : 252it [03:14,  1.29it/s, v_num=0]\n",
      "Epoch 0: : 253it [03:15,  1.29it/s, v_num=0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-07 17:38:50,431\tWARNING tune.py:192 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: : 254it [03:16,  1.29it/s, v_num=0]\n",
      "Epoch 0: : 255it [03:17,  1.29it/s, v_num=0]\n",
      "Epoch 0: : 256it [03:18,  1.29it/s, v_num=0]\n",
      "Epoch 0: : 257it [03:19,  1.29it/s, v_num=0]\n",
      "Epoch 0: : 258it [03:19,  1.29it/s, v_num=0]\n",
      "Epoch 0: : 259it [03:20,  1.29it/s, v_num=0]\n",
      "Epoch 0: : 260it [03:21,  1.29it/s, v_num=0]\n",
      "Epoch 0: : 261it [03:22,  1.29it/s, v_num=0]\n",
      "Epoch 0: : 262it [03:22,  1.29it/s, v_num=0]\n",
      "Epoch 0: : 263it [03:23,  1.29it/s, v_num=0]\n",
      "Epoch 0: : 264it [03:24,  1.29it/s, v_num=0]\n",
      "Epoch 0: : 265it [03:25,  1.29it/s, v_num=0]\n",
      "Epoch 0: : 266it [03:26,  1.29it/s, v_num=0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-07 17:39:00,443\tINFO tune.py:1148 -- Total run time: 1153.77 seconds (1143.66 seconds for the tuning loop).\n",
      "2023-09-07 17:39:00,444\tWARNING tune.py:1158 -- Training has been interrupted, but the most recent state was saved.\n",
      "Resume training with: Trainer.restore(path=\"/home/mpp/ray_results/ptl-sent-classification\", ...)\n",
      "2023-09-07 17:39:00,447\tWARNING experiment_analysis.py:916 -- Failed to read the results for 1 trials:\n",
      "- /home/mpp/ray_results/ptl-sent-classification/LightningTrainer_11eaa_00000_0_2023-09-07_17-19-46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(LightningTrainer pid=12028)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=12028)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=12028)\u001b[0m   File \"/home/mpp/miniconda3/envs/ray-torch/lib/python3.9/site-packages/ray/_private/function_manager.py\", line 726, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=12028)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=12028)\u001b[0m   File \"/home/mpp/miniconda3/envs/ray-torch/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 464, in _resume_span\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=12028)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=12028)\u001b[0m   File \"/home/mpp/miniconda3/envs/ray-torch/lib/python3.9/site-packages/ray/tune/trainable/trainable.py\", line 372, in train\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=12028)\u001b[0m     result = self.step()\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=12028)\u001b[0m   File \"/home/mpp/miniconda3/envs/ray-torch/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 464, in _resume_span\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=12028)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=12028)\u001b[0m   File \"/home/mpp/miniconda3/envs/ray-torch/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 389, in step\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=12028)\u001b[0m     result = self._results_queue.get(\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=12028)\u001b[0m   File \"/home/mpp/miniconda3/envs/ray-torch/lib/python3.9/queue.py\", line 180, in get\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=12028)\u001b[0m     self.not_empty.wait(remaining)\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=12028)\u001b[0m   File \"/home/mpp/miniconda3/envs/ray-torch/lib/python3.9/threading.py\", line 316, in wait\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=12028)\u001b[0m     gotit = waiter.acquire(True, timeout)\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=12028)\u001b[0m   File \"/home/mpp/miniconda3/envs/ray-torch/lib/python3.9/site-packages/ray/_private/worker.py\", line 776, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=12028)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=12028)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=12028)\u001b[0m \n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=12028)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=12028)\u001b[0m \n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=12028)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=12028)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1418, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=12028)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1501, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=12028)\u001b[0m   File \"/home/mpp/miniconda3/envs/ray-torch/lib/python3.9/site-packages/ray/_private/worker.py\", line 569, in record_task_log_end\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=12028)\u001b[0m     self.core_worker.record_task_log_end(\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=12028)\u001b[0m AttributeError: 'Worker' object has no attribute 'core_worker'\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=12028)\u001b[0m \n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=12028)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=12028)\u001b[0m \n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=12028)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=12028)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1787, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=12028)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1684, in ray._raylet.execute_task_with_cancellation_handler\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=12028)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1366, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=12028)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1367, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=12028)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1583, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=12028)\u001b[0m   File \"python/ray/_raylet.pyx\", line 813, in ray._raylet.store_task_errors\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=12028)\u001b[0m AttributeError: 'Worker' object has no attribute 'core_worker'\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=12028)\u001b[0m \n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=12028)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=12028)\u001b[0m \n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=12028)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=12028)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1824, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=12028)\u001b[0m   File \"/home/mpp/miniconda3/envs/ray-torch/lib/python3.9/site-packages/ray/_private/utils.py\", line 174, in push_error_to_driver\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=12028)\u001b[0m     worker.core_worker.push_error(job_id, error_type, message, time.time())\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=12028)\u001b[0m AttributeError: 'Worker' object has no attribute 'core_worker'\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=12028)\u001b[0m Exception ignored in: 'ray._raylet.task_execution_handler'\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=12028)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=12028)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1824, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=12028)\u001b[0m   File \"/home/mpp/miniconda3/envs/ray-torch/lib/python3.9/site-packages/ray/_private/utils.py\", line 174, in push_error_to_driver\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=12028)\u001b[0m     worker.core_worker.push_error(job_id, error_type, message, time.time())\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=12028)\u001b[0m AttributeError: 'Worker' object has no attribute 'core_worker'\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=12071)\u001b[0m   File \"/home/mpp/miniconda3/envs/ray-torch/lib/python3.9/site-packages/ray/train/_internal/worker_group.py\", line 29, in __execute\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=12071)\u001b[0m     return func(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=12071)\u001b[0m   File \"/home/mpp/miniconda3/envs/ray-torch/lib/python3.9/site-packages/ray/train/_internal/backend_executor.py\", line 471, in get_next\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=12071)\u001b[0m     result = session.get_next()\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=12071)\u001b[0m   File \"/home/mpp/miniconda3/envs/ray-torch/lib/python3.9/site-packages/ray/train/_internal/session.py\", line 201, in get_next\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=12071)\u001b[0m     result = self.result_queue.get(\n"
     ]
    }
   ],
   "source": [
    "trainer = LightningTrainer(\n",
    "    lightning_config=lightning_config,\n",
    "    run_config=run_config,\n",
    "    scaling_config=scaling_config,\n",
    "    datasets={\"train\": ray_datasets[\"train\"], \"val\": ray_datasets[\"validation\"]},\n",
    "    datasets_iter_config={\"batch_size\": 32},\n",
    "    preprocessor=preprocessor,\n",
    ")\n",
    "result = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Result(\n",
       "  metrics={'_report_on': 'validation_end', 'train_loss': 0.08543746918439865, 'matthews_correlation': 0.5930452712523209, 'epoch': 4, 'step': 2675, 'should_checkpoint': True, 'done': True, 'trial_id': '9082c_00000', 'experiment_tag': '0'},\n",
       "  path='/home/dino/ray_results/ptl-sent-classification/LightningTrainer_9082c_00000_0_2023-09-06_23-51-03',\n",
       "  checkpoint=LightningCheckpoint(local_path=/home/dino/ray_results/ptl-sent-classification/LightningTrainer_9082c_00000_0_2023-09-06_23-51-03/checkpoint_000004)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ray",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
