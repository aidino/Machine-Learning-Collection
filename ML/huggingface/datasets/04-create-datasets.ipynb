{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create image datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ImageFolder` automatically infers the class labels of your dataset based on the directory name. Store your dataset in a directory structure like:\n",
    "\n",
    "```bash\n",
    "folder/train/dog/golden_retriever.png\n",
    "folder/train/dog/german_shepherd.png\n",
    "folder/train/dog/chihuahua.png\n",
    "\n",
    "folder/train/cat/maine_coon.png\n",
    "folder/train/cat/bengal.png\n",
    "folder/train/cat/birman.png\n",
    "```\n",
    "\n",
    "Then users can load your `dataset` by specifying `imagefolder` in `load_dataset()` and the directory in `data_dir`:\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"imagefolder\", data_dir=\"/path/to/folder\")\n",
    "```\n",
    "\n",
    "You can also use `imagefolder` to load datasets involving multiple splits.\n",
    "\n",
    "```bash\n",
    "folder/train/dog/golden_retriever.png\n",
    "folder/train/cat/maine_coon.png\n",
    "folder/test/dog/german_shepherd.png\n",
    "folder/test/cat/bengal.png\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is additional information youâ€™d like to include about your dataset, like text captions or bounding boxes, add it as a `metadata.csv` file in your folder\n",
    "\n",
    "```bash\n",
    "folder/train/metadata.csv\n",
    "folder/train/0001.png\n",
    "folder/train/0002.png\n",
    "folder/train/0003.png\n",
    "```\n",
    "\n",
    "You can also zip your images:\n",
    "\n",
    "```bash\n",
    "folder/metadata.csv\n",
    "folder/train.zip\n",
    "folder/test.zip\n",
    "folder/valid.zip\n",
    "```\n",
    "\n",
    "Your `metadata.csv` file must have a `file_name` column which links image files with their metadata:\n",
    "\n",
    "```bash\n",
    "file_name,additional_feature\n",
    "0001.png,This is a first value of a text feature you added to your images\n",
    "0002.png,This is a second value of a text feature you added to your images\n",
    "0003.png,This is a third value of a text feature you added to your images\n",
    "```\n",
    "\n",
    "or using `metadata.jsonl`:\n",
    "\n",
    "```json\n",
    "{\"file_name\": \"0001.png\", \"additional_feature\": \"This is a first value of a text feature you added to your images\"}\n",
    "{\"file_name\": \"0002.png\", \"additional_feature\": \"This is a second value of a text feature you added to your images\"}\n",
    "{\"file_name\": \"0003.png\", \"additional_feature\": \"This is a third value of a text feature you added to your images\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image captioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image captioning datasets have text describing an image. An example `metadata.csv` may look like:\n",
    "\n",
    "```bash\n",
    "file_name,text\n",
    "0001.png,This is a golden retriever playing with a ball\n",
    "0002.png,A german shepherd\n",
    "0003.png,One chihuahua\n",
    "```\n",
    "Load the dataset with `ImageFolder`, and it will create a text column for the image captions:\n",
    "\n",
    "```bash\n",
    "dataset = load_dataset(\"imagefolder\", data_dir=\"/path/to/folder\", split=\"train\")\n",
    "dataset[0][\"text\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Object detection datasets have bounding boxes and categories identifying objects in an image. An example `metadata.jsonl` may look like:\n",
    "\n",
    "```jsonl\n",
    "{\"file_name\": \"0001.png\", \"objects\": {\"bbox\": [[302.0, 109.0, 73.0, 52.0]], \"categories\": [0]}}\n",
    "{\"file_name\": \"0002.png\", \"objects\": {\"bbox\": [[810.0, 100.0, 57.0, 28.0]], \"categories\": [1]}}\n",
    "{\"file_name\": \"0003.png\", \"objects\": {\"bbox\": [[160.0, 31.0, 248.0, 616.0], [741.0, 68.0, 202.0, 401.0]], \"categories\": [2, 2]}}\n",
    "```\n",
    "\n",
    "Load the dataset with `ImageFolder`, and it will create a objects column with the bounding boxes and the categories:\n",
    "\n",
    "```json\n",
    "dataset = load_dataset(\"imagefolder\", data_dir=\"/path/to/folder\", split=\"train\")\n",
    "dataset[0][\"objects\"]\n",
    "```\n",
    "\n",
    "return \n",
    "\n",
    "```bash\n",
    "{\"bbox\": [[302.0, 109.0, 73.0, 52.0]], \"categories\": [0]}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, ðŸ¤— Datasets samples a text file `line by line` to build the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"text\", data_files={\"train\": [\"my_text_1.txt\", \"my_text_2.txt\"], \"test\": \"my_test_file.txt\"})\n",
    "\n",
    "dataset = load_dataset(\"text\", data_dir=\"path/to/text/dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sample a text file by `paragraph` or even an `entire document`, use the `sample_by parameter`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"text\", data_files={\"train\": \"my_train_file.txt\", \"test\": \"my_test_file.txt\"}, sample_by=\"paragraph\")\n",
    "\n",
    "dataset = load_dataset(\"text\", data_files={\"train\": \"my_train_file.txt\", \"test\": \"my_test_file.txt\"}, sample_by=\"document\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use `grep patterns` to load specific files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "c4_subset = load_dataset(\"allenai/c4\", data_files=\"en/c4-train.0000*-of-01024.json.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load remote text files via HTTP, pass the URLs instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"text\", data_files=\"https://huggingface.co/datasets/lhoestq/test/resolve/main/some_text.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda examples: tokenizer(examples[\"text\"]), batched=True)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```json\n",
    "{'text': 'the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .', \n",
    " 'label': 1, \n",
    " 'input_ids': [101, 1996, 2600, 2003, 16036, 2000, 2022, 1996, 7398, 2301, 1005, 1055, 2047, 1000, 16608, 1000, 1998, 2008, 2002, 1005, 1055, 2183, 2000, 2191, 1037, 17624, 2130, 3618, 2084, 7779, 29058, 8625, 13327, 1010, 3744, 1011, 18856, 19513, 3158, 5477, 4168, 2030, 7112, 16562, 2140, 1012, 102], \n",
    " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \n",
    " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda examples: tokenizer(examples[\"text\"], return_tensors=\"np\"), batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Align\n",
    "\n",
    "The `align_labels_with_mapping()` function aligns a dataset label id with the label name. \n",
    "\n",
    "Not all ðŸ¤— Transformers models follow the prescribed label mapping of the original dataset, especially for `NLI` datasets. \n",
    "\n",
    "For example, the `MNLI` dataset uses the following label mapping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "mnli = load_dataset(\"glue\", \"mnli\", split=\"train\")\n",
    "mnli_aligned = mnli.align_labels_with_mapping(label2id, \"label\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ray-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
