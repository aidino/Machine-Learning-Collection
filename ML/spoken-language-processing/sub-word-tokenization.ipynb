{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subword tokenization\n",
    "\n",
    "> Subword tokenization algorithms rely on the principle that **frequently used words should not be split into smaller subwords**, but **rare words should be decomposed\n",
    "> into meaningful subwords**. \n",
    "\n",
    "For instance \"annoyingly\" might be considered a rare word and could be decomposed into \"annoying\" and \"ly\". Both \"annoying\" and \"ly\" as stand-alone subwords would appear more frequently while at the same time the meaning of \"annoyingly\" is kept by the composite meaning of \"annoying\" and \"ly\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Byte pair encoding (BPE)\n",
    "\n",
    "- BPE relies on a pre-tokenizer that splits the training data into words\n",
    "- After pre-tokenization, a set of unique words has been created and the frequency with which each word occurred in the training data has been determined\n",
    "- Next, BPE creates a base vocabulary consisting of all symbols that occur in the set of unique words and learns merge rules to form a new symbol from two symbols of the base vocabulary\n",
    "\n",
    "Example:\n",
    "\n",
    "- let’s assume that after pre-tokenization, the following set of words including their frequency has been determined:\n",
    "\n",
    "`(\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5)`\n",
    "\n",
    "- Splitting all words into symbols of the base vocabulary, we obtain:\n",
    "\n",
    "`(\"h\" \"u\" \"g\", 10), (\"p\" \"u\" \"g\", 5), (\"p\" \"u\" \"n\", 12), (\"b\" \"u\" \"n\", 4), (\"h\" \"u\" \"g\" \"s\", 5)`\n",
    "\n",
    "- BPE then counts the frequency of each possible **symbol pair** and picks the symbol pair that occurs **most frequently**. \n",
    "\n",
    "In the example above `\"h\"` followed by `\"u\"` is present `10 + 5 = 15 times` (10 times in the 10 occurrences of \"hug\", 5 times in the 5 occurrences of \"hugs\").\n",
    "\n",
    "However, the most frequent symbol pair is `\"u\"` followed by `\"g\"`, occurring `10 + 5 + 5 = 20 times` in total.\n",
    "\n",
    "Thus, the first merge rule the tokenizer learns is to group all `\"u\"` symbols followed by a `\"g\"` symbol together. Next, `\"ug\"` is added to the vocabulary. The set of words then becomes:\n",
    "\n",
    "`(\"h\" \"ug\", 10), (\"p\" \"ug\", 5), (\"p\" \"u\" \"n\", 12), (\"b\" \"u\" \"n\", 4), (\"h\" \"ug\" \"s\", 5)`\n",
    "\n",
    "- BPE then identifies the next most common symbol pair. It’s `\"u\"` followed by `\"n\"`, which occurs 16 times. `\"u\"`, `\"n\"` is merged to `\"un\"` and added to the vocabulary. The next most frequent symbol pair is `\"h\"` followed by `\"ug\"`, occurring 15 times. Again the pair is merged and `\"hug\"` can be added to the vocabulary.\n",
    "\n",
    "`(\"hug\", 10), (\"p\" \"ug\", 5), (\"p\" \"un\", 12), (\"b\" \"un\", 4), (\"hug\" \"s\", 5)`\n",
    "\n",
    "Assuming, that the Byte-Pair Encoding training would stop at this point, the learned merge rules would then be applied to new words \n",
    "\n",
    "For instance, the word `\"bug\"` would be tokenized to `[\"b\", \"ug\"]` but `\"mug\"` would be tokenized as `[\"<unk>\", \"ug\"]` since the symbol `\"m\"` is not in the base vocabulary.\n",
    "\n",
    "- the vocabulary size, i.e. the **base vocabulary size** + **the number of merges**, is a hyperparameter to choose. \n",
    "\n",
    "For instance GPT has a vocabulary size of `40,478` since they have `478` base characters and chose to stop training after `40,000` merges.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordPiece\n",
    "\n",
    "- WordPiece is the subword tokenization algorithm used for BERT, DistilBERT, and Electra.\n",
    "- The algorithm is the same with BPE but there is a difference:\n",
    "  - WordPiece does not choose the most frequent symbol pair, but the one that **maximizes the likelihood** of the training data once added to the vocabulary.\n",
    "\n",
    " E.g. `\"u\"`, followed by `\"g\"` would have only been merged if the probability of `\"ug\"` divided by `\"u\"`, `\"g\"` would have been greater than for any other symbol pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram\n",
    "\n",
    "- In contrast to **BPE** or **WordPiece**, **Unigram** initializes its base vocabulary to a large number of symbols and progressively trims down each symbol to obtain a smaller vocabulary. \n",
    "- `Unigram` is not used directly for any of the models in the transformers, but it’s used in conjunction with `SentencePiece`.\n",
    "- At each training step, the `Unigram` algorithm defines a loss (often defined as the log-likelihood) over the training data given the current vocabulary and a unigram language model. \n",
    "- Then, for each symbol in the vocabulary, the algorithm computes `how much the overall loss would increase` if the symbol was to be removed from the vocabulary. \n",
    "- Unigram then removes `p` (with `p` usually being `10%` or `20%`) percent of the symbols whose loss increase is the lowest\n",
    "- This process is repeated until the vocabulary has reached the desired size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SentencePiece\n",
    "\n",
    "- All tokenization algorithms described so far have the same problem: It is assumed that the input text uses **spaces** to separate words.\n",
    "- However, not all languages use spaces to separate words.\n",
    "- One possible solution is to use language specific pre-tokenizers, e.g. XLM uses a specific Chinese, Japanese, and Thai pre-tokenizer).\n",
    "\n",
    "=> To solve this problem more generally, SentencePiece treats the input as a raw input stream, thus including the space in the set of characters to use.\n",
    "It then uses the BPE or unigram algorithm to construct the appropriate vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
