{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization pipeline\n",
    "\n",
    "When calling `Tokenizer.encode` or `Tokenizer.encode_batch`, the input text(s) go through the following pipeline:\n",
    "\n",
    "- normalization\n",
    "- pre-tokenization\n",
    "- model\n",
    "- post-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "`Normalization` is a set of operations you apply to a raw string to make it less random or ‚Äúcleaner‚Äù."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import normalizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NFD, NFKD, NFC, NFKC\n",
    "\n",
    "l√† qu√° tr√¨nh ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ ƒë·∫£m b·∫£o r·∫±ng c√°c vƒÉn b·∫£n Unicode t∆∞∆°ng ƒë∆∞∆°ng ƒë∆∞·ª£c bi·ªÉu di·ªÖn m·ªôt c√°ch th·ªëng nh·∫•t v√† nh·∫•t qu√°n. Unicode x√°c ƒë·ªãnh c√°ch bi·ªÉu di·ªÖn k√Ω t·ª± gi·ªëng nhau theo nhi·ªÅu c√°ch kh√°c nhau, v√† vi·ªác chu·∫©n h√≥a gi√∫p ƒë·∫£m b·∫£o r·∫±ng vi·ªác x·ª≠ l√Ω v√† so s√°nh vƒÉn b·∫£n tr·ªü n√™n ƒë√°ng tin c·∫≠y v√† d·ªÖ d·ª± ƒëo√°n h∆°n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ChaÃÄo em coÃÇ gaÃÅi Lam HoÃÇÃÄng'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nfd = normalizers.NFD()\n",
    "nfd.normalize_str(\"Ch√†o em c√¥ g√°i Lam H·ªìng\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ChaÃÄo em coÃÇ gaÃÅi Lam HoÃÇÃÄng'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nfkd = normalizers.NFKD()\n",
    "nfkd.normalize_str(\"Ch√†o em c√¥ g√°i Lam H·ªìng\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowercase\n",
    "\n",
    "Replaces all uppercase to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ch√†o em c√¥ g√°i lam h·ªìng'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizers.Lowercase().normalize_str(\"Ch√†o em c√¥ g√°i Lam H·ªìng\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strip\n",
    "\n",
    "Removes all whitespace characters on the specified sides (left, right or both) of the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ch√†o em c√¥ g√°i Lam H·ªìng'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizers.Strip().normalize_str(\"   Ch√†o em c√¥ g√°i Lam H·ªìng   \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StripAccents\n",
    "\n",
    "Removes all accent symbols in unicode (to be used with NFD for consistency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chao em co gai Lam Hong'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers.normalizers import NFD, StripAccents\n",
    "normalizer = normalizers.Sequence([NFD(), StripAccents()])\n",
    "\n",
    "normalizer.normalize_str(\"Ch√†o em c√¥ g√°i Lam H·ªìng\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace\n",
    "\n",
    "Replaces a custom string or regexp and changes it with given content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ch√†o em c√¥ g√°i Lam H·ªìng '"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import Regex\n",
    "\n",
    "normalizer = normalizers.Replace('123', '')\n",
    "\n",
    "normalizer.normalize_str(\"Ch√†o em c√¥ g√°i Lam H·ªìng 123\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-tokenizers\n",
    "\n",
    "The PreTokenizer takes care of splitting the input according to a set of rules. \n",
    "\n",
    "You can easily combine multiple PreTokenizer together using a Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import pre_tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chaÃÄo em coÃÇ gaÃÅi lam hoÃÇÃÄng!'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers.normalizers import NFD, Lowercase\n",
    "normalizer = normalizers.Sequence([NFD(), Lowercase()])\n",
    "\n",
    "text_normalized = normalizer.normalize_str(\"Ch√†o em c√¥ g√°i Lam H·ªìng! 123 d√¥\")\n",
    "text_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ByteLevel\n",
    "\n",
    "Splits on whitespaces while remapping all the bytes to a set of visible characters. This technique as been introduced by OpenAI with GPT-2 and has some more or less nice properties:\n",
    "\n",
    "- Since it maps on bytes, a tokenizer using this only requires 256 characters as initial alphabet (the number of values a byte can have), as opposed to the 130,000+ Unicode characters.\n",
    "- A consequence of the previous point is that it is absolutely unnecessary to have an unknown token using this since we can represent anything with 256 tokens (Youhou!! üéâüéâ)\n",
    "- For non ascii characters, it gets completely unreadable, but it works nonetheless!\n",
    "\n",
    "Example:\n",
    "- Input: \"Hello my friend, how are you?\"\n",
    "- Ouput: \"Hello\", \"ƒ†my\", ƒ†friend\", \",\", \"ƒ†how\", \"ƒ†are\", \"ƒ†you\", \"?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ƒ†cha', (0, 3)),\n",
       " ('√åƒ¢', (3, 4)),\n",
       " ('o', (4, 5)),\n",
       " ('ƒ†em', (5, 8)),\n",
       " ('ƒ†co', (8, 11)),\n",
       " ('√åƒ§', (11, 12)),\n",
       " ('ƒ†ga', (12, 15)),\n",
       " ('√åƒ£', (15, 16)),\n",
       " ('i', (16, 17)),\n",
       " ('ƒ†lam', (17, 21)),\n",
       " ('ƒ†ho', (21, 24)),\n",
       " ('√åƒ§√åƒ¢', (24, 26)),\n",
       " ('ng', (26, 28)),\n",
       " ('!', (28, 29))]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "\n",
    "pre_tokenizer = ByteLevel(add_prefix_space=True, use_regex=True)\n",
    "pre_tokenizer.pre_tokenize_str(text_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whitespace\n",
    "\n",
    "Splits on word boundaries (using the following regular expression: `\\w+|[^\\w\\s]+`\n",
    "\n",
    "Example:\n",
    "- Input: \"Hello there!\"\n",
    "- Output: \"Hello\", \"there\", \"!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('chaÃÄo', (0, 5)),\n",
       " ('em', (6, 8)),\n",
       " ('coÃÇ', (9, 12)),\n",
       " ('gaÃÅi', (13, 17)),\n",
       " ('lam', (18, 21)),\n",
       " ('hoÃÇÃÄng', (22, 28)),\n",
       " ('!', (28, 29))]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "pre_tokenizer = Whitespace()\n",
    "pre_tokenizer.pre_tokenize_str(text_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WhitespaceSplit\n",
    "\n",
    "Splits on any whitespace characterSplits on any whitespace character\n",
    "\n",
    "Example:\n",
    "- Input: \"Hello there!\"\n",
    "- Output: \"Hello\", \"there!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('chaÃÄo', (0, 5)),\n",
       " ('em', (6, 8)),\n",
       " ('coÃÇ', (9, 12)),\n",
       " ('gaÃÅi', (13, 17)),\n",
       " ('lam', (18, 21)),\n",
       " ('hoÃÇÃÄng!', (22, 29))]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers.pre_tokenizers import WhitespaceSplit\n",
    "\n",
    "pre_tokenizer = WhitespaceSplit()\n",
    "pre_tokenizer.pre_tokenize_str(text_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuation\n",
    "\n",
    "Will isolate all punctuation characters\n",
    "\n",
    "`Punctuation( behavior = 'isolated' )`\n",
    "\n",
    "**behavior** (SplitDelimiterBehavior) ‚Äî The behavior to use when splitting. Choices: ‚Äúremoved‚Äù, ‚Äúisolated‚Äù (default), ‚Äúmerged_with_previous‚Äù, ‚Äúmerged_with_next‚Äù, ‚Äúcontiguous‚Äù\n",
    "\n",
    "Example:\n",
    "\n",
    "- Input: \"Hello?\"\n",
    "- Ouput: \"Hello\", \"?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('chaÃÄo em coÃÇ gaÃÅi lam hoÃÇÃÄng', (0, 28)), ('!', (28, 29))]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers.pre_tokenizers import Punctuation\n",
    "\n",
    "pre_tokenizer = Punctuation()\n",
    "pre_tokenizer.pre_tokenize_str(text_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metaspace\n",
    "\n",
    "`( replacement = '_', add_prefix_space = True )`\n",
    "\n",
    "\n",
    "Splits on whitespaces and replaces them with a special char ‚Äú‚ñÅ‚Äù (U+2581)\n",
    "\n",
    "Example: \n",
    "- Input: \"Hello there\"\n",
    "- Ouput: \"Hello\", \"‚ñÅthere\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('chaÃÄo', (0, 5)),\n",
       " ('‚ñÅem', (5, 8)),\n",
       " ('‚ñÅcoÃÇ', (8, 12)),\n",
       " ('‚ñÅgaÃÅi', (12, 17)),\n",
       " ('‚ñÅlam', (17, 21)),\n",
       " ('‚ñÅhoÃÇÃÄng!', (21, 29))]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers.pre_tokenizers import Metaspace\n",
    "\n",
    "pre_tokenizer = Metaspace(add_prefix_space = False)\n",
    "pre_tokenizer.pre_tokenize_str(text_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CharDelimiterSplit\n",
    "\n",
    "Splits on a given character\n",
    "\n",
    "- Example with x:\n",
    "- Input: \"Helloxthere\"\n",
    "- Ouput: \"Hello\", \"there\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hello', (0, 5)), ('world', (6, 11))]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers.pre_tokenizers import CharDelimiterSplit \n",
    "\n",
    "pre_tokenizer = CharDelimiterSplit('_')\n",
    "pre_tokenizer.pre_tokenize_str(\"hello_world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Digits\n",
    "\n",
    "`( individual_digits = False )`\n",
    "\n",
    "Splits the numbers from any other characters.\n",
    "\n",
    "- Input: \"Hello123there\"\n",
    "- Output: \"Hello\", \"123\", \"there\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hello world ', (0, 12)), ('1', (12, 13)), ('2', (13, 14)), ('3', (14, 15))]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers.pre_tokenizers import Digits\n",
    "pre_tokenizer = Digits(individual_digits = True)\n",
    "pre_tokenizer.pre_tokenize_str(\"hello world 123\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hello world ', (0, 12)), ('123', (12, 15))]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers.pre_tokenizers import Digits\n",
    "pre_tokenizer = Digits(individual_digits = False)\n",
    "pre_tokenizer.pre_tokenize_str(\"hello world 123\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split\n",
    "\n",
    "`( pattern, behavior,invert = False )`\n",
    "\n",
    "Versatile (Linh ho·∫°t) pre-tokenizer that splits on provided pattern and according to provided behavior. The pattern can be inverted if necessary.\n",
    "\n",
    "pattern should be either a custom string or regexp.\n",
    "\n",
    "behavior should be one of:\n",
    "- removed\n",
    "- isolated\n",
    "- merged_with_previous\n",
    "- merged_with_next\n",
    "- contiguous\n",
    "\n",
    "\n",
    "invert should be a boolean flag.\n",
    "\n",
    "Example with `pattern = `, `behavior = \"isolated\"`, `invert = False`:\n",
    "- Input: \"Hello, how are you?\"\n",
    "- Output: \"Hello,\", \" \", \"how\", \" \", \"are\", \" \", \"you?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence\n",
    "\n",
    "Lets you compose multiple PreTokenizer that will be run in the given order\n",
    "\n",
    "`Sequence([Punctuation(), WhitespaceSplit()])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('chaÃÄo', (0, 5)),\n",
       " ('em', (6, 8)),\n",
       " ('coÃÇ', (9, 12)),\n",
       " ('gaÃÅi', (13, 17)),\n",
       " ('lam', (18, 21)),\n",
       " ('hoÃÇÃÄng', (22, 28)),\n",
       " ('!', (28, 29))]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import pre_tokenizers\n",
    "from tokenizers.pre_tokenizers import Digits\n",
    "pre_tokenizer = pre_tokenizers.Sequence([Whitespace(), Digits(individual_digits=True)])\n",
    "pre_tokenized_str = pre_tokenizer.pre_tokenize_str(text_normalized)\n",
    "pre_tokenized_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "- `models.BPE`\n",
    "- `models.Unigram`\n",
    "- `models.WordLevel`\n",
    "- `models.WordPiece`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Processors\n",
    "\n",
    "After the whole pipeline, we sometimes want to insert some special tokens before feed a tokenized string into a model like ‚Äù`[CLS]` My horse is amazing `[SEP]`‚Äù. The PostProcessor is the component doing just that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TemplateProcessing\n",
    "\n",
    "Example, when specifying a template with these values:\n",
    "\n",
    "- single: \"[CLS] $A [SEP]\"\n",
    "- pair: \"[CLS] $A [SEP] $B [SEP]\"\n",
    "- special tokens:\n",
    "    - \"[CLS]\"\n",
    "    - \"[SEP]\"\n",
    "\n",
    "----\n",
    "- Input: (\"I like this\", \"but not this\")\n",
    "- Output: \"[CLS] I like this [SEP] but not this [SEP]\"\n",
    "\n",
    "----\n",
    "\n",
    "```python\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[(\"[CLS]\", 1), (\"[SEP]\", 2)],\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All together: a BERT tokenizer from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, BERT relies on WordPiece, so we instantiate a new Tokenizer with this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "bert_tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we know that BERT preprocesses texts by removing accents and lowercasing. We also use a unicode normalizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import NFD, Lowercase, StripAccents\n",
    "bert_tokenizer.normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pre-tokenizer is just splitting on whitespace and punctuation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "bert_tokenizer.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the post-processing uses the template we saw in the previous section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.processors import TemplateProcessing\n",
    "bert_tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", 1),\n",
    "        (\"[SEP]\", 2),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this tokenizer and train on it on wikitext like in the quicktour:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "No such file or directory (os error 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/mpp/Documents/Machine-Learning-Collection/ML/spoken-language-processing/tokenization-pipeline.ipynb Cell 51\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/mpp/Documents/Machine-Learning-Collection/ML/spoken-language-processing/tokenization-pipeline.ipynb#Y103sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m trainer \u001b[39m=\u001b[39m WordPieceTrainer(vocab_size\u001b[39m=\u001b[39m\u001b[39m30522\u001b[39m, special_tokens\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39m[UNK]\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m[CLS]\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m[SEP]\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m[PAD]\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m[MASK]\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/mpp/Documents/Machine-Learning-Collection/ML/spoken-language-processing/tokenization-pipeline.ipynb#Y103sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m files \u001b[39m=\u001b[39m [\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdata/wikitext-103-raw/wiki.\u001b[39m\u001b[39m{\u001b[39;00msplit\u001b[39m}\u001b[39;00m\u001b[39m.raw\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m split \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mvalid\u001b[39m\u001b[39m\"\u001b[39m]]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/mpp/Documents/Machine-Learning-Collection/ML/spoken-language-processing/tokenization-pipeline.ipynb#Y103sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m bert_tokenizer\u001b[39m.\u001b[39;49mtrain(files, trainer)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/mpp/Documents/Machine-Learning-Collection/ML/spoken-language-processing/tokenization-pipeline.ipynb#Y103sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m bert_tokenizer\u001b[39m.\u001b[39msave(\u001b[39m\"\u001b[39m\u001b[39mdata/bert-wiki.json\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: No such file or directory (os error 2)"
     ]
    }
   ],
   "source": [
    "from tokenizers.trainers import WordPieceTrainer\n",
    "trainer = WordPieceTrainer(vocab_size=30522, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "files = [f\"data/wikitext-103-raw/wiki.{split}.raw\" for split in [\"test\", \"train\", \"valid\"]]\n",
    "bert_tokenizer.train(files, trainer)\n",
    "bert_tokenizer.save(\"data/bert-wiki.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tokenizer.encode(\"Hello, y'all! How are you üòÅ ?\")\n",
    "print(output.ids)\n",
    "# [1, 27253, 16, 93, 11, 5097, 5, 7961, 5112, 6218, 0, 35, 2]\n",
    "tokenizer.decode([1, 27253, 16, 93, 11, 5097, 5, 7961, 5112, 6218, 0, 35, 2])\n",
    "# \"Hello , y ' all ! How are you ?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you used a model that added special characters to represent subtokens of a given ‚Äúword‚Äù (like the \"##\" in WordPiece) you will need to customize the decoder to treat them properly. If we take our previous bert_tokenizer for instance the default decoding will give:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = bert_tokenizer.encode(\"Welcome to the ü§ó Tokenizers library.\")\n",
    "print(output.tokens)\n",
    "# [\"[CLS]\", \"welcome\", \"to\", \"the\", \"[UNK]\", \"tok\", \"##eni\", \"##zer\", \"##s\", \"library\", \".\", \"[SEP]\"]\n",
    "bert_tokenizer.decode(output.ids)\n",
    "# \"welcome to the tok ##eni ##zer ##s library .\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But by changing it to a proper decoder, we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import decoders\n",
    "bert_tokenizer.decoder = decoders.WordPiece()\n",
    "bert_tokenizer.decode(output.ids)\n",
    "# \"welcome to the tokenizers library.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
